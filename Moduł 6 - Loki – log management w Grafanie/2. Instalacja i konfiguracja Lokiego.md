# Instalacja i konfiguracja Lokiego

Poniżej przedstawiono proces instalacji i konfiguracji Lokiego na systemie Ubuntu 24.04 LTS. Omówiono dwie metody instalacji: z pliku binarnego oraz przy użyciu Docker.

## Wymagania systemowe

### Minimalne wymagania:

- **System operacyjny**: Ubuntu 24.04 LTS (lub nowszy)
- **RAM**: Minimum 512 MB (zalecane 2 GB)
- **CPU**: 1 rdzeń (zalecane 2 rdzenie)
- **Dysk**: 10 GB wolnego miejsca
- **Sieć**: Dostęp do Internetu

### Wymagania dodatkowe:

- Uprawnienia **root** lub **sudo**
- Zainstalowany **systemd** (domyślnie w Ubuntu)
- Opcjonalnie: **Docker** (dla instalacji kontenerowej)

## Metody instalacji

Dostępne metody instalacji Lokiego:

1. **Binarka** – instalacja z gotowych plików binarnych
2. **Docker** – uruchomienie w kontenerze
3. **Kod źródłowy** – kompilacja z repozytorium GitHub
4. **Helm** – instalacja w Kubernetes

Poniżej szczegółowo omówiono instalację z binarki oraz Docker.

---

## Instalacja Lokiego z binarki

### Krok 1: Przygotowanie systemu

Aktualizacja systemu i instalacja niezbędnych narzędzi:

```bash
# Aktualizacja listy pakietów
sudo apt update

# Aktualizacja systemu
sudo apt upgrade -y

# Instalacja niezbędnych narzędzi
sudo apt install -y wget unzip curl
```

### Krok 2: Pobranie Lokiego

Pobranie najnowszej wersji Lokiego z oficjalnego repozytorium GitHub:

```bash
# Sprawdzenie najnowszej wersji: https://github.com/grafana/loki/releases
# Przykład dla wersji 2.9.3

# Przejście do katalogu tymczasowego
cd /tmp

# Pobranie Lokiego wersji 2.9.3
wget https://github.com/grafana/loki/releases/download/v2.9.3/loki-linux-amd64.zip

# Rozpakowanie archiwum
unzip loki-linux-amd64.zip

# Przeniesienie binarki do /usr/local/bin
sudo mv loki-linux-amd64 /usr/local/bin/loki

# Nadanie uprawnień wykonywania
sudo chmod +x /usr/local/bin/loki
```

**Wyjaśnienie:**
- `/usr/local/bin/` – standardowa lokalizacja dla aplikacji instalowanych ręcznie
- `chmod +x` – nadaje uprawnienia do wykonywania pliku

### Krok 3: Weryfikacja instalacji

Sprawdzenie poprawności instalacji:

```bash
# Sprawdzenie wersji Lokiego
loki --version
```

Oczekiwany wynik:

```
loki, version 2.9.3 (branch: HEAD, revision: e24f5ab7a2b)
  build user:       root@buildkitsandbox
  build date:       2023-11-15T15:32:45Z
  go version:       go1.21.4
  platform:         linux/amd64
```

### Krok 4: Utworzenie użytkownika systemowego

Utworzenie dedykowanego użytkownika systemowego dla Lokiego:

```bash
# Utworzenie dedykowanego użytkownika dla Lokiego
sudo useradd --system --no-create-home --shell /bin/false loki
```

**Wyjaśnienie:**
- `--system` – tworzy użytkownika systemowego (UID < 1000)
- `--no-create-home` – nie tworzy katalogu domowego
- `--shell /bin/false` – użytkownik nie może się logować interaktywnie

### Krok 5: Utworzenie katalogów danych

Utworzenie struktur katalogów dla danych Lokiego:

```bash
# Utworzenie katalogów dla danych Lokiego (WAŻNE: z katalogiem WAL!)
sudo mkdir -p /var/lib/loki/{index,cache,chunks,wal,compactor}

# Utworzenie katalogu konfiguracyjnego
sudo mkdir -p /etc/loki

# Nadanie uprawnień dla katalogów danych
sudo chown -R loki:loki /var/lib/loki

# Nadanie uprawnień dla katalogu konfiguracyjnego
sudo chown -R loki:loki /etc/loki
```

**Wyjaśnienie struktur katalogów:**
- `/etc/loki/` – pliki konfiguracyjne
- `/var/lib/loki/index` – indeksy danych
- `/var/lib/loki/cache` – cache dla indeksów
- `/var/lib/loki/chunks` – dane logów (chunki)
- `/var/lib/loki/wal` – Write-Ahead Log (WAL) dla ingester
- `/var/lib/loki/compactor` – katalog roboczy kompaktora

### Krok 6: Utworzenie pliku konfiguracyjnego

Podstawowy plik konfiguracyjny Lokiego:

```bash
sudo nano /etc/loki/loki-config.yaml
```

Zawartość pliku konfiguracyjnego (WAŻNE: z konfiguracją WAL!):

> **⚠️ Uwaga o WAL:** Poniższa konfiguracja zawiera **WAL (Write-Ahead Log)** - mechanizm zapewniający, że żadne logi nie zostaną utracone w przypadku awarii lub restartu Lokiego. WAL zapisuje logi najpierw na dysk, zanim trafią do pamięci RAM. To kluczowa funkcja dla środowisk produkcyjnych. Szczegółowe wyjaśnienie WAL znajduje się poniżej w sekcji "Czym jest WAL".

```yaml
# Podstawowa konfiguracja Lokiego
auth_enabled: false

# Konfiguracja serwera HTTP
server:
  http_listen_port: 3100
  grpc_listen_port: 9096
  log_level: info

# Konfiguracja ingester (komponent przyjmujący logi)
ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
  chunk_idle_period: 5m
  chunk_retain_period: 30s
  max_transfer_retries: 0
  # Konfiguracja WAL (Write-Ahead Log)
  wal:
    enabled: true
    dir: /var/lib/loki/wal

# Konfiguracja schematu przechowywania danych
schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

# Konfiguracja przechowywania danych
storage_config:
  boltdb_shipper:
    active_index_directory: /var/lib/loki/index
    cache_location: /var/lib/loki/cache
    shared_store: filesystem
  filesystem:
    directory: /var/lib/loki/chunks

# Limity i ograniczenia
limits_config:
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  ingestion_rate_mb: 10
  ingestion_burst_size_mb: 20

# Konfiguracja kompaktora (czyszczenie starych danych)
compactor:
  working_directory: /var/lib/loki/compactor
  shared_store: filesystem

# Retention (opcjonalne - usuwanie starych logów)
chunk_store_config:
  max_look_back_period: 0s

table_manager:
  retention_deletes_enabled: false
  retention_period: 0s
```

### Szczegółowe wyjaśnienie pliku konfiguracyjnego

Poniżej przedstawiono szczegółowy opis każdej sekcji i parametru w pliku konfiguracyjnym Loki.

---

#### Sekcja 1: Podstawowa konfiguracja

```yaml
auth_enabled: false
```

**auth_enabled** (Autentykacja włączona)

- **Typ:** Boolean (true/false)
- **Domyślnie:** false
- **Opis:** Włącza lub wyłącza mechanizm multi-tenancy (wielodostępności) z autentykacją.

**Wartości:**
- `false` - **Tryb single-tenant** (jeden użytkownik/organizacja)
  - Nie wymaga nagłówka `X-Scope-OrgID` w żądaniach
  - Wszystkie logi trafiają do jednej "puli"
  - Idealne dla środowisk testowych i małych wdrożeń
  - **Używamy tej wartości w naszej konfiguracji**

- `true` - **Tryb multi-tenant** (wiele organizacji)
  - WYMAGA nagłówka `X-Scope-OrgID` w każdym żądaniu
  - Logi są izolowane między organizacjami
  - Używane w dużych wdrożeniach z wieloma zespołami
  - Przykład: `curl -H "X-Scope-OrgID: team-frontend" http://localhost:3100/loki/api/v1/push`

**Kiedy zmienić na true:**
- Masz wiele zespołów/działów używających jednego Loki
- Potrzebujesz izolacji danych między organizacjami
- Chcesz osobne limity dla różnych użytkowników

---

#### Sekcja 2: Konfiguracja serwera HTTP

```yaml
server:
  http_listen_port: 3100
  grpc_listen_port: 9096
  log_level: info
```

**http_listen_port** (Port HTTP)

- **Typ:** Integer (numer portu)
- **Domyślnie:** 3100
- **Opis:** Port na którym Loki nasłuchuje żądań HTTP/REST API.

**Używany do:**
- Push logów od Promtail: `POST /loki/api/v1/push`
- Zapytania LogQL: `GET /loki/api/v1/query`
- Health check: `GET /ready`
- Metryki Prometheusa: `GET /metrics`
- Konfiguracja: `GET /config`

**Przykłady użycia:**
```bash
# Health check
curl http://localhost:3100/ready

# Zapytanie o logi
curl -G http://localhost:3100/loki/api/v1/query --data-urlencode 'query={job="audit"}'
```

**Zmiana portu:**
Jeśli port 3100 jest zajęty, możesz zmienić na inny (np. 3200). Pamiętaj aby zaktualizować:
- Konfigurację Promtail (clients.url)
- Konfigurację Data Source w Grafanie
- Reguły firewall

---

**grpc_listen_port** (Port gRPC)

- **Typ:** Integer (numer portu)
- **Domyślnie:** 9096
- **Opis:** Port dla komunikacji gRPC między komponentami Loki i agentami.

**Używany do:**
- Komunikacja Promtail ↔ Loki (alternatywa dla HTTP)
- Komunikacja między rozproszonymi komponentami Loki
- Szybszy protokół niż HTTP (binary protocol)

**Kiedy jest używany:**
- Promtail skonfigurowany do używania gRPC zamiast HTTP
- Rozproszona instalacja Loki (ingester, distributor, querier na osobnych serwerach)
- Wymagana większa wydajność (gRPC jest szybszy od HTTP)

**Uwaga:** W naszej podstawowej konfiguracji Promtail używa HTTP (port 3100), więc port 9096 może być nieużywany. Można go wyłączyć ustawiając `grpc_listen_port: 0`.

---

**log_level** (Poziom logowania)

- **Typ:** String
- **Domyślnie:** info
- **Opis:** Określa poziom szczegółowości logów samego Loki (nie logów aplikacji, tylko działania Loki).

**Dostępne wartości:**
- `debug` - Najbardziej szczegółowe, używane do debugowania problemów
  - Pokazuje każde zapytanie, każdy chunk, każdą operację
  - Generuje DUŻO logów
  - Używaj tylko tymczasowo do diagnozowania błędów

- `info` - **Zalecane dla produkcji**
  - Wystarczający poziom do monitorowania
  - Pokazuje ważne zdarzenia (start, shutdown, błędy)
  - Nie generuje nadmiernej ilości logów

- `warn` - Tylko ostrzeżenia i błędy
  - Cichszy niż info
  - Może przeoczyć ważne informacje

- `error` - Tylko błędy krytyczne
  - Bardzo cichy
  - Używaj tylko jeśli logi Loki są problemem

**Gdzie oglądać logi Loki:**
```bash
# Logi z systemd
sudo journalctl -u loki -f

# Logi z ostatniej godziny
sudo journalctl -u loki --since "1 hour ago"

# Tylko błędy
sudo journalctl -u loki -p err
```

---

#### Sekcja 3: Konfiguracja ingester (komponent przyjmujący logi)

```yaml
ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
  chunk_idle_period: 5m
  chunk_retain_period: 30s
  max_transfer_retries: 0
  wal:
    enabled: true
    dir: /var/lib/loki/wal
```

**Ingester** to komponent Loki odpowiedzialny za:
- Przyjmowanie logów od Promtail
- Trzymanie ich w pamięci RAM
- Grupowanie w "chunki" (porcje skompresowanych logów)
- Zapisywanie chunków na dysk

---

**lifecycler.address** (Adres ingester)

- **Typ:** String (adres IP)
- **Domyślnie:** 127.0.0.1
- **Opis:** Adres IP na którym ingester się ogłasza w "ring" (pierścieniu ingestersów).

**Wartości:**
- `127.0.0.1` - Instalacja jednowęzłowa (tylko lokalnie)
- `0.0.0.0` - Nasłuchuje na wszystkich interfejsach
- Konkretny IP (np. `10.0.0.15`) - W klastrze z wieloma ingestersami

**W naszej konfiguracji:** `127.0.0.1` oznacza że mamy tylko jeden ingester działający lokalnie.

---

**ring.kvstore.store** (Magazyn metadanych ring)

- **Typ:** String
- **Domyślnie:** inmemory
- **Opis:** Gdzie przechowywać informacje o dostępnych ingestersach w klastrze.

**Dostępne wartości:**
- `inmemory` - **W pamięci RAM** (nasza konfiguracja)
  - Najprostsze
  - Dla jednego węzła
  - Dane znikają po restarcie (ale to OK, ring się odtworzy)
  
- `consul` - Consul (system service discovery)
  - Dla klastra wielu węzłów
  - Persystentne przechowywanie
  - Wymaga uruchomionego Consul
  
- `etcd` - etcd (distributed key-value store)
  - Dla klastra
  - Używane w Kubernetes
  
- `memberlist` - Gossip protocol
  - Ingester komunikują się między sobą
  - Nie wymaga zewnętrznego systemu

**Kiedy zmienić:** Jeśli budujesz klaster z wieloma serwerami Loki.

---

**replication_factor** (Współczynnik replikacji)

- **Typ:** Integer
- **Domyślnie:** 1
- **Opis:** Ile kopii danych przechowywać w klastrze.

**Wartości:**
- `1` - **Bez replikacji** (nasza konfiguracja)
  - Jeden ingester, jedna kopia danych
  - Jeśli ingester padnie → dane w RAM są stracone (ale WAL chroni!)
  - Używaj dla jednego węzła
  
- `3` - **Trzy kopie** (zalecane dla klastra)
  - Każdy log jest zapisany na 3 różnych ingestersach
  - Wysoka dostępność
  - Można stracić 2 węzły i wciąż mieć dane
  
- `2` - Dwie kopie (kompromis)

**Wzór:** W klastrze powinieneś mieć co najmniej `replication_factor` ingestersów.

**Przykład:** Jeśli masz 5 ingestersów i `replication_factor: 3`, każdy log jest na 3 z 5 serwerów.

---

**chunk_idle_period** (Okres bezczynności chunka)

- **Typ:** Duration (czas)
- **Domyślnie:** 30m (30 minut)
- **Nasza wartość:** 5m (5 minut)
- **Opis:** Jak długo chunk może być "bezczynny" (brak nowych logów) zanim zostanie zamknięty i zapisany na dysk.

**Proces:**
1. Logi przychodzą → trafiają do "otwartego" chunka w RAM
2. Jeśli przez 5 minut nie przyjdą nowe logi do tego strumienia
3. Chunk jest zamykany, kompresowany i zapisywany na dysk

**Niższa wartość (np. 5m):**
- ✅ Szybsze uwalnianie pamięci RAM
- ✅ Logi szybciej trafiają na dysk (mniejsze okno utraty przy awarii bez WAL)
- ❌ Więcej małych chunków = więcej operacji I/O

**Wyższa wartość (np. 30m):**
- ✅ Mniej chunków = lepsza kompresja
- ✅ Mniej operacji zapisu na dysk
- ❌ Więcej RAM zużywanego przez otwarte chunki
- ❌ Większe okno potencjalnej utraty danych (bez WAL)

**Zalecenie:** 5-15 minut dla większości przypadków.

---

**chunk_retain_period** (Okres zatrzymania chunka)

- **Typ:** Duration (czas)
- **Domyślnie:** 0s
- **Nasza wartość:** 30s (30 sekund)
- **Opis:** Jak długo trzymać chunk w pamięci **AFTER** zamknięciu, zanim zostanie zapisany na dysk.

**Po co to?**
Pozwala na późne przychodzące logi (out-of-order logs). Jeśli log z timestampem 10:00:00 przyjdzie o 10:00:35, a chunk dla 10:00:xx został zamknięty o 10:00:30, log jeszcze może trafić do tego chunka.

**Wartości:**
- `0s` - Natychmiast zapisz po zamknięciu (szybsze zwalnianie RAM)
- `30s` - **Zalecane** - bufor dla spóźnionych logów
- `5m` - Dla systemów z dużymi opóźnieniami sieciowymi

**Uwaga:** Im dłużej, tym więcej RAM używane.

---

**max_transfer_retries** (Maksymalna liczba prób transferu)

- **Typ:** Integer
- **Domyślnie:** 10
- **Nasza wartość:** 0
- **Opis:** Ile razy próbować przenieść dane między ingestersami podczas "hand-off" (przekazywania).

**Kiedy jest używane:**
- W klastrze z wieloma ingestersami
- Gdy ingester odchodzi (shutdown) i przekazuje swoje dane innemu

**Nasza wartość 0:**
- Nie próbujemy transferu
- OK dla single-node (jeden węzeł)
- W klastrze powinieneś ustawić 3-10

---

**wal.enabled** (WAL włączony)

- **Typ:** Boolean
- **Domyślnie:** false
- **Nasza wartość:** true ✅
- **Opis:** Czy włączyć Write-Ahead Log (szczegółowe wyjaśnienie w sekcji "Czym jest WAL" powyżej).

**Funkcja:**
- Zapisuje logi na dysk **PRZED** trzymaniem w RAM
- Chroni przed utratą danych przy awarii
- **ZAWSZE włączaj w produkcji!**

---

**wal.dir** (Katalog WAL)

- **Typ:** String (ścieżka)
- **Domyślnie:** /var/lib/loki/wal
- **Opis:** Gdzie przechowywać pliki WAL.

**Wymagania:**
- Katalog musi istnieć
- Użytkownik `loki` musi mieć uprawnienia write
- Wystarczająca przestrzeń dyskowa (zwykle 100-500 MB)

**Monitoring rozmiaru:**
```bash
du -sh /var/lib/loki/wal
```

---

#### Sekcja 4: Konfiguracja schematu (schema_config)

```yaml
schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h
```

**Schema config** definiuje JAK Loki przechowuje i indeksuje dane. Możesz mieć wiele schematów dla różnych okresów czasu.

---

**from** (Od daty)

- **Typ:** Date (YYYY-MM-DD)
- **Nasza wartość:** 2020-10-24
- **Opis:** Od jakiej daty stosować ten schemat.

**Przykład użycia wielu schematów:**
```yaml
schema_config:
  configs:
    # Stary schemat (legacy)
    - from: 2020-01-01
      store: boltdb
      schema: v9
    
    # Nowy schemat (od października 2020)
    - from: 2020-10-24
      store: boltdb-shipper
      schema: v11
```

**Dlaczego 2020-10-24:**
To data wprowadzenia `boltdb-shipper` jako domyślnego store. Jest to standardowa wartość startowa dla nowych instalacji.

**Zmiana schematu:**
- NIE zmieniaj `from` dla istniejących danych!
- Jeśli chcesz zmienić schemat, dodaj nowy wpis z nową datą
- Stare dane pozostają w starym schemacie

---

**store** (Typ magazynu indeksów)

- **Typ:** String
- **Nasza wartość:** boltdb-shipper ✅
- **Opis:** Jaki backend używać do przechowywania indeksów.

**Dostępne opcje:**
- `boltdb-shipper` - **Zalecane, nowoczesne** ✅
  - Wbudowana baza danych (BoltDB)
  - Nie wymaga zewnętrznych serwisów
  - Automatyczne "shipping" indeksów do object store
  - Idealne dla małych i średnich wdrożeń
  
- `boltdb` - Stara wersja
  - Wymaga współdzielonego systemu plików (NFS)
  - Deprecated (przestarzałe)
  
- `cassandra` - Apache Cassandra
  - Dla bardzo dużych wdrożeń
  - Wymaga klastra Cassandra
  
- `bigtable` - Google Cloud Bigtable
  - Dla wdrożeń w GCP
  - Kosztowne
  
- `dynamodb` - AWS DynamoDB
  - Dla wdrożeń w AWS

**Zalecenie:** Zostań przy `boltdb-shipper` - jest prosty i wydajny.

---

**object_store** (Magazyn obiektów)

- **Typ:** String
- **Nasza wartość:** filesystem ✅
- **Opis:** Gdzie przechowywać chunki (skompresowane dane logów).

**Dostępne opcje:**
- `filesystem` - **Lokalny dysk** ✅
  - Najprostsze
  - Dla jednego węzła lub współdzielonego NFS
  - Ścieżka: `/var/lib/loki/chunks/`
  
- `s3` - Amazon S3 (lub kompatybilne: MinIO, Ceph)
  - Dla dużych wdrożeń
  - Nieograniczona przestrzeń
  - Wymaga konfiguracji AWS credentials
  
- `gcs` - Google Cloud Storage
  - Dla wdrożeń w GCP
  
- `azure` - Azure Blob Storage
  - Dla wdrożeń w Azure

**Kiedy zmienić:**
Jeśli potrzebujesz:
- Nieograniczonej przestrzeni
- Wysokiej dostępności (S3 ma 11 dziewiątek durability)
- Rozdzielenia compute od storage

---

**schema** (Wersja schematu)

- **Typ:** String
- **Nasza wartość:** v11 ✅
- **Opis:** Wersja formatu danych Loki.

**Historia wersji:**
- `v9` - Stara wersja (przed 2020)
- `v10` - Wprowadzenie boltdb-shipper
- `v11` - **Aktualna, zalecana** ✅
- `v12` - Eksperymentalna (TSDB - Time Series Database)

**Zalecenie:** Używaj `v11` - jest stabilny i dobrze przetestowany.

---

**index.prefix** (Prefix indeksu)

- **Typ:** String
- **Domyślnie:** index_
- **Opis:** Prefix dla plików indeksów w storage.

**Pliki na dysku będą nazwane:**
```
/var/lib/loki/index/
├── index_18945  (dzień 2022-01-15)
├── index_18946  (dzień 2022-01-16)
├── index_18947  (dzień 2022-01-17)
```

**Po co to:**
- Organizacja plików
- Możliwość wielu instalacji Loki w jednym bucket
- Rozdzielenie środowisk: `index_prod_`, `index_dev_`

**Zmiana:** Tylko jeśli masz specyficzne wymagania.

---

**index.period** (Okres indeksu)

- **Typ:** Duration
- **Nasza wartość:** 24h (1 dzień) ✅
- **Opis:** Jak często tworzyć nowy plik indeksu.

**Dostępne wartości:**
- `24h` - **Zalecane** ✅
  - Jeden indeks na dzień
  - Łatwe zarządzanie retention
  - Optymalny dla większości przypadków
  
- `168h` (7 dni) - Jeden indeks na tydzień
  - Mniej plików
  - Trudniejsze usuwanie starych danych
  
- `12h` - Dwa indeksy dziennie
  - Więcej plików
  - Używaj tylko jeśli masz OGROMNE ilości logów

**Dlaczego 24h jest najlepsze:**
- Retention ustawia się w dniach (łatwo usunąć cały dzień)
- Nie za dużo, nie za mało plików
- Standardowa praktyka

---

#### Sekcja 5: Konfiguracja przechowywania (storage_config)

```yaml
storage_config:
  boltdb_shipper:
    active_index_directory: /var/lib/loki/index
    cache_location: /var/lib/loki/cache
    shared_store: filesystem
  filesystem:
    directory: /var/lib/loki/chunks
```

**Storage config** określa GDZIE fizycznie są przechowywane pliki.

---

**boltdb_shipper.active_index_directory**

- **Typ:** String (ścieżka)
- **Nasza wartość:** /var/lib/loki/index
- **Opis:** Katalog gdzie Loki trzyma AKTYWNE indeksy (bieżący okres).

**Jak to działa:**
1. Loki zapisuje indeks dla bieżącego okresu (dzisiaj) w tym katalogu
2. Po zakończeniu okresu (o północy) indeks jest "wysyłany" do `shared_store`
3. Lokalne pliki są usuwane (opcjonalnie cache)

**Zawartość:**
```bash
ls -lh /var/lib/loki/index/
# Zobaczysz pliki typu:
# index_18945/
#   compactor-xxx.gz
#   ingester-xxx.gz
```

**Wymagania:**
- Szybki dysk (SSD zalecane)
- ~1-5 GB przestrzeni
- Uprawnienia dla użytkownika `loki`

---

**boltdb_shipper.cache_location**

- **Typ:** String (ścieżka)
- **Nasza wartość:** /var/lib/loki/cache
- **Opis:** Katalog na cache pobranych indeksów ze `shared_store`.

**Funkcja:**
- Przy zapytaniach do starych danych Loki pobiera indeksy z shared_store
- Cache przyspiesza kolejne zapytania do tych samych okresów
- Automatycznie zarządzany (stare pliki są usuwane)

**Rozmiar:**
- Domyślnie: kilkaset MB do kilku GB
- Rośnie z czasem ale ma limity
- Można czyścić ręcznie jeśli potrzeba: `rm -rf /var/lib/loki/cache/*` (tylko przy zatrzymanym Loki!)

---

**boltdb_shipper.shared_store**

- **Typ:** String
- **Nasza wartość:** filesystem
- **Opis:** Gdzie "wysyłać" zakończone indeksy.

**Wartości:**
- `filesystem` - Lokalny system plików ✅
- `s3` - Amazon S3
- `gcs` - Google Cloud Storage
- `azure` - Azure Blob Storage

**Musi być zgodne z `object_store` w schema_config!**

---

**filesystem.directory**

- **Typ:** String (ścieżka)
- **Nasza wartość:** /var/lib/loki/chunks
- **Opis:** Główny katalog gdzie przechowywane są CHUNKI (skompresowane logi).

**Struktura katalogów:**
```bash
/var/lib/loki/chunks/
├── fake/              # Namespace (dla single-tenant: "fake")
│   ├── xxx/xxx/       # Organizacja według hash
│   │   ├── chunk1.gz  # Skompresowany chunk
│   │   ├── chunk2.gz
│   │   └── ...
```

**Rozmiar:**
- **To największy katalog!**
- Rośnie proporcjonalnie do ilości logów
- Z kompresją: ~10-30% oryginalnego rozmiaru logów

**Przykład:**
- 100 GB logów dziennie
- Kompresja: 5:1
- Po kompresji: 20 GB dziennie
- Retention 30 dni: ~600 GB

**Monitoring:**
```bash
# Sprawdź rozmiar
du -sh /var/lib/loki/chunks

# Najwięksi "jadacze" miejsca
du -h --max-depth=3 /var/lib/loki/chunks | sort -hr | head -20
```

---

#### Sekcja 6: Limity i ograniczenia (limits_config)

```yaml
limits_config:
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  ingestion_rate_mb: 10
  ingestion_burst_size_mb: 20
```

**Limits config** chroni Loki przed przeciążeniem i złymi konfiguracjami.

---

**reject_old_samples**

- **Typ:** Boolean
- **Nasza wartość:** true ✅
- **Opis:** Czy odrzucać logi ze starymi timestampami.

**Wartości:**
- `true` - **Zalecane** ✅
  - Odrzuca logi starsze niż `reject_old_samples_max_age`
  - Chroni przed błędnymi timestampami
  - Zapobiega problemom z indeksowaniem
  
- `false` - Akceptuj wszystkie logi
  - Pozwala na import historycznych logów
  - Może powodować problemy z performance

**Kiedy ustawić false:**
- Jednorazowy import starych logów
- Migracja z innego systemu
- **Pamiętaj zmienić z powrotem na true!**

---

**reject_old_samples_max_age**

- **Typ:** Duration
- **Nasza wartość:** 168h (7 dni) ✅
- **Opis:** Jak stare logi są akceptowane.

**Przykład:**
- Dzisiaj: 2026-02-13 16:00
- Max age: 168h (7 dni)
- Loki przyjmie logi z timestampem >= 2026-02-06 16:00
- Starsze ODRZUCI z błędem

**Kiedy zwiększyć:**
- Masz duże opóźnienia w sieci
- Systemy w różnych strefach czasowych z desynchronizacją
- Import historycznych danych

**Typowe wartości:**
- `24h` (1 dzień) - Restrykcyjne
- `168h` (7 dni) - **Zalecane** ✅
- `720h` (30 dni) - Dla importu

**Błąd przy odrzuceniu:**
```
entry with timestamp 2026-02-01 has been rejected because it's too old
```

---

**ingestion_rate_mb**

- **Typ:** Float (MB/s)
- **Nasza wartość:** 10 MB/s
- **Opis:** Maksymalna średnia szybkość przyjmowania logów (per tenant).

**Jak to działa:**
- Loki mierzy średnią w oknie (np. ostatnie 60 sekund)
- Jeśli przekroczysz limit → status HTTP 429 (Too Many Requests)
- Promtail musi czekać i retry

**Przykład:**
- Limit: 10 MB/s
- Wysyłasz 8 MB/s przez minutę → OK
- Nagle spike: 15 MB/s → ODRZUCONE przez ~5 sekund, potem OK

**Kiedy zwiększyć:**
- Widzisz błędy 429 w logach Promtail
- Generujesz więcej niż 10 MB/s logów
- Typowe wartości produkcyjne: 50-200 MB/s

**Monitoring:**
```bash
# Sprawdź metrykę
curl -s http://localhost:3100/metrics | grep loki_distributor_ingestion_rate_limit

# Sprawdź czy logi są odrzucane
curl -s http://localhost:3100/metrics | grep discarded_samples_total
```

---

**ingestion_burst_size_mb**

- **Typ:** Float (MB)
- **Nasza wartość:** 20 MB
- **Opis:** Jak duży "burst" (nagły wyskok) jest dozwolony.

**Różnica od rate:**
- `ingestion_rate_mb` - średnia w czasie (MB/s)
- `ingestion_burst_size_mb` - maksymalny rozmiar pojedynczego żądania

**Przykład:**
- Rate: 10 MB/s
- Burst: 20 MB
- Możesz wysłać 20 MB naraz
- Ale średnia w czasie musi pozostać <= 10 MB/s

**Typowa wartość:** 2x `ingestion_rate_mb`

**Kiedy zwiększyć:**
- Promtail wysyła duże batche logów
- Widzisz błędy o przekroczeniu burst size
- Importujesz dane partiami

---

#### Sekcja 7: Kompaktor (compactor)

```yaml
compactor:
  working_directory: /var/lib/loki/compactor
  shared_store: filesystem
```

**Compactor** to proces który:
- Agreguje małe pliki indeksów w większe
- Usuwa zduplikowane wpisy
- Wykonuje retention (usuwanie starych danych)
- Działa w tle, nie wpływa na zapytania

---

**working_directory**

- **Typ:** String (ścieżka)
- **Nasza wartość:** /var/lib/loki/compactor
- **Opis:** Katalog roboczy dla operacji kompakcji.

**Funkcja:**
- Tymczasowe pliki podczas kompakcji
- Automatycznie czyszczony po zakończeniu
- Potrzebne miejsce: ~2-5 GB

**Wymagania:**
- Szybki dysk (SSD zalecane)
- Uprawnienia dla użytkownika `loki`

---

**shared_store**

- **Typ:** String
- **Nasza wartość:** filesystem
- **Opis:** Ten sam shared_store co w boltdb_shipper.

**Musi być identyczny!** Kompaktor czyta i pisze do tego samego miejsca co indeksy.

---

#### Sekcja 8: Retention - usuwanie starych logów

```yaml
chunk_store_config:
  max_look_back_period: 0s

table_manager:
  retention_deletes_enabled: false
  retention_period: 0s
```

**Retention** to automatyczne usuwanie starych logów by nie zapełniać dysku.

---

**chunk_store_config.max_look_back_period**

- **Typ:** Duration
- **Nasza wartość:** 0s (wyłączone)
- **Opis:** Jak daleko wstecz można odpytywać logi.

**Wartości:**
- `0s` - **Bez limitu** (nasza konfiguracja)
  - Możesz zapytać o logi z dowolnej przeszłości
  - Jeśli istnieją fizycznie na dysku
  
- `720h` (30 dni) - Limit 30 dni
  - Zapytania starsze niż 30 dni zwrócą błąd
  - Nawet jeśli dane fizycznie istnieją!
  - Optymalizacja: Loki nie musi skanować bardzo starych indeksów

**Kiedy ustawić:**
- Masz retention 30 dni i nie chcesz pozwolić na zapytania o starsze dane
- Optymalizacja performance dla bardzo dużych instalacji

---

**table_manager.retention_deletes_enabled**

- **Typ:** Boolean
- **Nasza wartość:** false (wyłączone) ⚠️
- **Opis:** Czy automatycznie usuwać stare logi.

**Wartości:**
- `false` - **Retention wyłączony** ⚠️
  - Logi **NIE SĄ** automatycznie usuwane
  - Dysk będzie się zapełniał!
  - **Musisz ręcznie zarządzać miejscem**
  
- `true` - **Retention włączony** ✅
  - Automatyczne usuwanie logów starszych niż `retention_period`
  - Konieczne dla produkcji!

**UWAGA:** W naszej bazowej konfiguracji retention jest WYŁĄCZONY. To OK dla testów, ale w produkcji zmień na `true`!

---

**table_manager.retention_period**

- **Typ:** Duration
- **Nasza wartość:** 0s (nieaktywne)
- **Opis:** Jak długo przechowywać logi przed usunięciem.

**Przykłady:**
- `744h` (31 dni) - Usuwaj logi starsze niż 31 dni
- `2160h` (90 dni) - 3 miesiące retention
- `8760h` (365 dni) - Rok retention

**Jak działa:**
1. Kompaktor sprawdza indeksy codziennie
2. Znajduje okresy starsze niż retention_period
3. Usuwa:
   - Pliki indeksów
   - Chunki z logami
4. Zwalnia miejsce na dysku

**Włączenie retention (produkcja):**
```yaml
table_manager:
  retention_deletes_enabled: true
  retention_period: 744h  # 31 dni
```

**Monitoring:**
```bash
# Sprawdź czy kompaktor działa
curl -s http://localhost:3100/metrics | grep loki_compactor

# Sprawdź retention
curl -s http://localhost:3100/config | grep -A5 retention
```

---

### Podsumowanie najważniejszych parametrów

| Parametr | Nasza wartość | Kiedy zmienić |
|----------|---------------|---------------|
| `auth_enabled` | false | W multi-tenant (wiele zespołów) |
| `http_listen_port` | 3100 | Jeśli port zajęty |
| `log_level` | info | `debug` dla troubleshootingu |
| `wal.enabled` | **true** ✅ | **NIGDY nie wyłączaj w prod!** |
| `chunk_idle_period` | 5m | Więcej RAM? Zwiększ do 15-30m |
| `replication_factor` | 1 | W klastrze: 3 |
| `store` | boltdb-shipper | Zostaw |
| `object_store` | filesystem | Dla S3: zmień + dodaj AWS config |
| `schema` | v11 | Zostaw |
| `index.period` | 24h | Zostaw |
| `reject_old_samples_max_age` | 168h | Większe opóźnienia? Zwiększ |
| `ingestion_rate_mb` | 10 | Więcej logów? 50-200 MB/s |
| `retention_deletes_enabled` | **false** ⚠️ | **Zmień na true w prod!** |
| `retention_period` | 0s | **Ustaw np. 744h (31 dni)** |

**Kluczowe zmiany przed produkcją:**
```yaml
# 1. Włącz retention
table_manager:
  retention_deletes_enabled: true
  retention_period: 744h

# 2. Zwiększ limity jeśli potrzeba
limits_config:
  ingestion_rate_mb: 50  # Dostosuj do swoich potrzeb
  ingestion_burst_size_mb: 100

# 3. Upewnij się że WAL jest włączony
ingester:
  wal:
    enabled: true  # ✅ KRYTYCZNE!
```

#### Czym jest WAL (Write-Ahead Log)?

**WAL (Write-Ahead Log)** to mechanizm zapewniający **niezawodność i trwałość danych** w Loki. Jest to kluczowa funkcja dla środowisk produkcyjnych.

**Jak działa WAL:**

```
┌──────────────┐
│  Promtail    │ wysyła logi
└──────┬───────┘
       ↓
┌──────────────────────────────────────┐
│           Loki Ingester              │
│  ┌────────────────────────────────┐  │
│  │ 1. Zapisz do WAL (dysk)    ✓   │  │ ← Natychmiastowy zapis
│  │    /var/lib/loki/wal/          │  │
│  └────────────────────────────────┘  │
│  ┌────────────────────────────────┐  │
│  │ 2. Trzymaj w pamięci (RAM)     │  │
│  └────────────────────────────────┘  │
│  ┌────────────────────────────────┐  │
│  │ 3. Po czasie → zapisz do       │  │
│  │    chunków (kompresja)         │  │ ← Opóźniony zapis
│  │    /var/lib/loki/chunks/       │  │
│  └────────────────────────────────┘  │
└──────────────────────────────────────┘
```

**Krok po kroku:**

1. **Log przychodzi** od Promtail do Lokiego
2. **Natychmiast zapisany do WAL** na dysku (`/var/lib/loki/wal/`)
3. **Trzymany w pamięci RAM** ingester dla szybkiego dostępu
4. **Po pewnym czasie** (np. 5 minut) lub po osiągnięciu limitu rozmiaru:
   - Logi z pamięci są kompresowane
   - Zapisywane jako "chunki" do `/var/lib/loki/chunks/`
   - Wpis w WAL jest usuwany

**Dlaczego WAL jestważny:**

❌ **Bez WAL:**
```
Log → RAM → [AWARIA! Restart Loki] → Dane utracone ❌
```

✅ **Z WAL:**
```
Log → WAL (dysk) + RAM → [AWARIA! Restart Loki] 
    → Odczyt z WAL → Dane odzyskane ✓
```

**Scenariusze ochrony:**

1. **Awaria serwera** (np. nagła utrata zasilania)
   - Bez WAL: Logi w pamięci RAM (ostatnie 5-30 minut) są **tracone**
   - Z WAL: Po restarcie Loki odczytuje WAL i **odzyskuje** wszystkie logi

2. **Restart Lokiego** (celowy lub przez błąd)
   - Bez WAL: Dane w RAM znikają
   - Z WAL: Replay z WAL, zero utraty danych

3. **Błąd aplikacji** (crash procesu Loki)
   - Bez WAL: Wszystko w pamięci przepada
   - Z WAL: Automatyczny odczyt przy następnym starcie

**Koszt włączenia WAL:**

- ✅ **Zalety:**
  - Brak utraty danych
  - Szybkie odzyskiwanie po awarii
  - Bezpieczne restarty

- ⚠️ **Koszty:**
  - ~10-20% dodatkowych operacji I/O dysku
  - Zajmuje miejsce na dysku (zwykle niewiele, ~100-500 MB)
  - Logi są najpierw zapisywane do WAL, potem do chunków (podwójny zapis)

**Zalecenie:**

```yaml
# ✅ Środowisko PRODUKCYJNE
wal:
  enabled: true
  dir: /var/lib/loki/wal

# ⚠️ Tylko do TESTÓW/DEV (akceptujesz ryzyko utraty danych)
wal:
  enabled: false
```

**W produkcji ZAWSZE włączaj WAL!** Koszt wydajności jest minimalny, a ochrona przed utratą danych bezcenna.

**Monitorowanie WAL:**

Możesz monitorować stan WAL przez metryki Prometheusa:

```bash
# Rozmiar WAL
curl -s http://localhost:3100/metrics | grep loki_ingester_wal_disk_size_bytes

# Liczba segmentów WAL
curl -s http://localhost:3100/metrics | grep loki_ingester_wal_segments

# Liczba niezapisanych logów w WAL
curl -s http://localhost:3100/metrics | grep loki_ingester_wal_records_logged_total
```

**Czyszczenie WAL:**

WAL jest automatycznie czyszczony przez Loki:
- Po zapisaniu chunków do dysku
- Po osiągnięciu maksymalnego wieku WAL
- Po restarcie (jeśli dane zostały przeniesione do chunków)

**Nie usuwaj ręcznie plików z `/var/lib/loki/wal/` podczas działania Lokiego!**

### Krok 7: Utworzenie usługi systemd

Plik usługi systemd dla Lokiego:

```bash
sudo nano /etc/systemd/system/loki.service
```

Zawartość pliku usługi:

```ini
[Unit]
Description=Loki Log Aggregation System
Documentation=https://grafana.com/docs/loki/latest/
After=network.target

[Service]
Type=simple
User=loki
Group=loki
ExecStart=/usr/local/bin/loki -config.file=/etc/loki/loki-config.yaml
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
```

**Wyjaśnienie opcji usługi:**

- **Type**: Typ uruchomienia procesu. `simple` oznacza proces działający w foreground.
- **User/Group**: Użytkownik i grupa systemowa dla procesu.
- **ExecStart**: Komenda uruchamiająca Lokiego z określonym plikiem konfiguracyjnym.
- **Restart**: Polityka restartu. `on-failure` oznacza restart tylko po awarii.
- **RestartSec**: Czas oczekiwania przed restartem w sekundach. Domyślnie: 10.

### Krok 8: Uruchomienie usługi Loki

Przeładowanie konfiguracji systemd i uruchomienie usługi:

```bash
# Przeładowanie konfiguracji systemd
sudo systemctl daemon-reload

# Włączenie automatycznego uruchamiania przy starcie systemu
sudo systemctl enable loki

# Uruchomienie usługi Loki
sudo systemctl start loki

# Sprawdzenie statusu usługi
sudo systemctl status loki
```

Oczekiwany wynik:

```
● loki.service - Loki Log Aggregation System
     Loaded: loaded (/etc/systemd/system/loki.service; enabled; vendor preset: enabled)
     Active: active (running) since Wed 2026-02-05 10:30:15 UTC; 5s ago
       Docs: https://grafana.com/docs/loki/latest/
   Main PID: 12345 (loki)
      Tasks: 10 (limit: 4096)
     Memory: 45.2M
        CPU: 234ms
     CGroup: /system.slice/loki.service
             └─12345 /usr/local/bin/loki -config.file=/etc/loki/loki-config.yaml
```

### Krok 9: Weryfikacja działania

Sprawdzenie dostępności endpointów Lokiego:

```bash
# Sprawdzenie gotowości Loki
curl http://localhost:3100/ready

# Sprawdzenie informacji o wersji
curl -s http://localhost:3100/metrics | grep loki_build_info

# Sprawdzenie wszystkich metryk Lokiego
curl http://localhost:3100/metrics

# Sprawdzenie konfiguracji
curl http://localhost:3100/config
```

Poprawna odpowiedź z `/ready` to kod HTTP 200 z komunikatem "ready".

**Wyjaśnienie endpointów:**

- `/ready` – sprawdzenie, czy Loki jest gotowy do przyjmowania żądań
- `/metrics` – metryki Prometheusa eksportowane przez Lokiego
- `/config` – bieżąca konfiguracja Lokiego w formacie YAML

### Krok 10: Przegląd logów

Analiza logów Lokiego w systemd journal:

```bash
# Ostatnie 50 linii logów
sudo journalctl -u loki -n 50

# Logi w czasie rzeczywistym
sudo journalctl -u loki -f

# Logi z ostatniej godziny
sudo journalctl -u loki --since "1 hour ago"

# Logi z określonego okresu
sudo journalctl -u loki --since "2026-02-05 10:00:00" --until "2026-02-05 11:00:00"
```

---

## Instalacja Lokiego przez Docker

Alternatywna metoda instalacji z użyciem kontenera Docker.

### Krok 1: Instalacja Docker

Instalacja Docker Engine na Ubuntu 24.04:

```bash
# Instalacja wymaganych pakietów
sudo apt install -y ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings

# Dodanie klucza GPG Docker
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

# Dodanie repozytorium Docker
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Aktualizacja i instalacja Docker
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io

# Weryfikacja instalacji
sudo docker --version
```

### Krok 2: Przygotowanie struktury katalogów

Utworzenie katalogów dla danych i konfiguracji:

```bash
# Utworzenie katalogów dla Lokiego
mkdir -p ~/loki-docker/config
mkdir -p ~/loki-docker/data

# Przejście do katalogu roboczego
cd ~/loki-docker
```

### Krok 3: Utworzenie pliku konfiguracyjnego

Plik konfiguracyjny dla kontenera Docker (WAŻNE: z konfiguracją WAL!):

```bash
nano ~/loki-docker/config/loki-config.yaml
```

> **⚠️ Uwaga o WAL:** Poniższa konfiguracja zawiera **WAL (Write-Ahead Log)** - mechanizm zapewniający, że żadne logi nie zostaną utracone w przypadku awarii lub restartu kontenera. Szczegółowe wyjaśnienie WAL znajduje się w sekcji instalacji z binarki powyżej.

Zawartość pliku konfiguracyjnego:

```yaml
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096
  log_level: info

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
  chunk_idle_period: 5m
  chunk_retain_period: 30s
  max_transfer_retries: 0
  # Konfiguracja WAL (Write-Ahead Log)
  wal:
    enabled: true
    dir: /loki/wal

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /loki/index
    cache_location: /loki/cache
    shared_store: filesystem
  filesystem:
    directory: /loki/chunks

limits_config:
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  ingestion_rate_mb: 10
  ingestion_burst_size_mb: 20

compactor:
  working_directory: /loki/compactor
  shared_store: filesystem

chunk_store_config:
  max_look_back_period: 0s

table_manager:
  retention_deletes_enabled: false
  retention_period: 0s
```

**Uwaga:** Ścieżki w kontenerze używają `/loki/` zamiast `/var/lib/loki/`. Konfiguracja zawiera również WAL dla lepszej niezawodności danych.

### Krok 4: Uruchomienie kontenera Loki

Uruchomienie Lokiego jako kontenera Docker:

```bash
# Uruchomienie kontenera Loki
docker run -d \
  --name=loki \
  --restart=unless-stopped \
  -p 3100:3100 \
  -p 9096:9096 \
  -v ~/loki-docker/config:/etc/loki \
  -v ~/loki-docker/data:/loki \
  grafana/loki:2.9.3 \
  -config.file=/etc/loki/loki-config.yaml
```

**Wyjaśnienie parametrów:**

- `-d`: Uruchomienie w trybie detached (w tle).
- `--name=loki`: Nazwa kontenera.
- `--restart=unless-stopped`: Automatyczny restart kontenera po restarcie systemu.
- `-p 3100:3100`: Mapowanie portu HTTP hosta na port kontenera.
- `-p 9096:9096`: Mapowanie portu gRPC.
- `-v ~/loki-docker/config:/etc/loki`: Montowanie katalogu konfiguracji z hosta do kontenera.
- `-v ~/loki-docker/data:/loki`: Montowanie katalogu danych.
- `grafana/loki:2.9.3`: Obraz Docker z określoną wersją Lokiego.
- `-config.file=/etc/loki/loki-config.yaml`: Ścieżka do pliku konfiguracyjnego w kontenerze.

### Krok 5: Weryfikacja kontenera

Sprawdzenie działania kontenera:

```bash
# Sprawdzenie statusu kontenera
docker ps | grep loki

# Wyświetlenie logów kontenera
docker logs loki

# Wyświetlenie logów w czasie rzeczywistym
docker logs -f loki

# Sprawdzenie endpoint /ready
curl http://localhost:3100/ready

# Sprawdzenie metryk
curl http://localhost:3100/metrics
```

### Krok 6: Zarządzanie kontenerem

Podstawowe operacje na kontenerze:

```bash
# Zatrzymanie kontenera
docker stop loki

# Uruchomienie kontenera
docker start loki

# Restart kontenera
docker restart loki

# Wyświetlenie statystyk zasobów
docker stats loki

# Usunięcie kontenera
docker rm -f loki
```

---

## Zaawansowana konfiguracja

### 1. Konfiguracja retencji logów

Konfiguracja automatycznego usuwania starych logów:

```yaml
# Dodanie do limits_config
limits_config:
  retention_period: 744h  # 31 dni

# Włączenie retencji w table_manager
table_manager:
  retention_deletes_enabled: true
  retention_period: 744h
```

**Wyjaśnienie:**
- **retention_period**: Okres przechowywania logów w godzinach.
- **retention_deletes_enabled**: Włączenie automatycznego usuwania starych logów.

### 2. Włączenie autentykacji

Konfiguracja multi-tenancy z autentykacją:

```yaml
auth_enabled: true

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

# Wymagany nagłówek HTTP: X-Scope-OrgID
# Przykład: X-Scope-OrgID: tenant1
```

**Wyjaśnienie:**

Każde żądanie do Lokiego musi zawierać nagłówek `X-Scope-OrgID` identyfikujący tenant (organizację).

Przykład użycia:

```bash
curl -H "X-Scope-OrgID: tenant1" http://localhost:3100/loki/api/v1/query
```

### 3. Optymalizacja wydajności

Konfiguracja dla większych obciążeń:

```yaml
# Zwiększone limity dla większych obciążeń
limits_config:
  ingestion_rate_mb: 50
  ingestion_burst_size_mb: 100
  max_streams_per_user: 10000
  max_global_streams_per_user: 100000

# Optymalizacja ingester
ingester:
  chunk_encoding: snappy
  chunk_target_size: 1572864  # 1.5 MB
  max_chunk_age: 2h
```

**Wyjaśnienie parametrów:**

- **ingestion_rate_mb**: Maksymalna szybkość przyjmowania logów w MB/s.
- **ingestion_burst_size_mb**: Maksymalny rozmiar burstu w MB.
- **max_streams_per_user**: Maksymalna liczba unikalnych strumieni na użytkownika.
- **chunk_encoding**: Algorytm kompresji chunków. `snappy` zapewnia szybką kompresję.
- **chunk_target_size**: Docelowy rozmiar chunka w bajtach.
- **max_chunk_age**: Maksymalny wiek chunka przed jego zamknięciem.

### 4. Konfiguracja z zewnętrznym storage (S3)

Użycie S3-compatible storage dla większych wdrożeń:

```yaml
storage_config:
  aws:
    s3: s3://region/bucket_name
    access_key_id: ${AWS_ACCESS_KEY_ID}
    secret_access_key: ${AWS_SECRET_ACCESS_KEY}
    bucketnames: loki-chunks
  boltdb_shipper:
    active_index_directory: /loki/index
    cache_location: /loki/cache
    shared_store: s3
```

**Wyjaśnienie:**

Zewnętrzne storage (S3, GCS, Azure) umożliwia:
- Większą skalowalność przechowywania
- Oddzielenie storage od compute
- Łatwiejsze skalowanie poziome

---

## Integracja z Promtail - zbieranie logów auditd

Promtail to agent odpowiedzialny za zbieranie logów i wysyłanie ich do Lokiego. Poniżej przedstawiono konfigurację integracji logów **auditd** (Linux Audit Daemon) z Loki poprzez Promtail.

### Dlaczego Promtail?

Loki nie czyta plików logów bezpośrednio - wymaga agenta, który:
- Monitoruje pliki logów w czasie rzeczywistym
- Dodaje etykiety (labels) niezbędne do indeksowania
- Parsuje i przetwarza logi przez pipeline
- Wysyła dane do Lokiego przez HTTP API
- Obsługuje rotację logów automatycznie

### Architektura przepływu danych

```
┌─────────────┐
│   Auditd    │ → zapisuje logi do /var/log/audit/audit.log
└─────────────┘
       ↓
┌─────────────┐
│  Promtail   │ → czyta, parsuje, dodaje etykiety
└─────────────┘
       ↓ (HTTP push)
┌─────────────┐
│    Loki     │ → indeksuje według etykiet, przechowuje
└─────────────┘
       ↓ (LogQL query)
┌─────────────┐
│   Grafana   │ → wizualizuje dane jako wykresy i alerty
└─────────────┘
```

### Instalacja Promtail

```bash
# Pobranie Promtail (wersja 2.9.3 - zgodna z Loki)
cd /tmp
wget https://github.com/grafana/loki/releases/download/v2.9.3/promtail-linux-amd64.zip

# Rozpakowanie
unzip promtail-linux-amd64.zip

# Instalacja
sudo mv promtail-linux-amd64 /usr/local/bin/promtail
sudo chmod +x /usr/local/bin/promtail

# Weryfikacja
promtail --version
```

### Konfiguracja Promtail dla auditd

Utworzenie pliku konfiguracyjnego:

```bash
sudo mkdir -p /etc/promtail
sudo nano /etc/promtail/promtail-config.yaml
```

**Zawartość pliku konfiguracyjnego:**

```yaml
# Konfiguracja serwera Promtail
server:
  http_listen_port: 9080  # Port dla metryk i health check
  grpc_listen_port: 0     # Wyłączenie gRPC

# Gdzie wysyłać logi
clients:
  - url: http://localhost:3100/loki/api/v1/push
    # Endpoint Lokiego dla push logów

# Konfiguracja zbierania logów
scrape_configs:
  # Job dla auditd
  - job_name: audit
    static_configs:
      - targets:
          - localhost
        labels:
          job: audit
          source: auditd
          __path__: /var/log/audit/audit.log
    
    # Pipeline przetwarzania logów
    pipeline_stages:
      # Stage 1: Parsowanie logów auditd przez regex
      - regex:
          expression: 'type=(?P<audit_type>\w+).*msg=audit\((?P<timestamp>[^)]+)\).*key="(?P<audit_key>[^"]+)"'
      
      # Stage 2: Tworzenie etykiet z wyparsowanych wartości
      - labels:
          audit_type:
          audit_key:
```

### Wyjaśnienie konfiguracji

#### Sekcja `server`

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0
```

- **http_listen_port**: Port na którym Promtail udostępnia metryki (`/metrics`) i endpoint health check (`/ready`)
- **grpc_listen_port**: Ustawione na 0 - wyłącza komunikację gRPC (nie jest potrzebna dla podstawowej konfiguracji)

#### Sekcja `clients`

```yaml
clients:
  - url: http://localhost:3100/loki/api/v1/push
```

- **url**: Endpoint Lokiego, na który Promtail wysyła logi
- Format: `http://<adres-loki>:<port>/loki/api/v1/push`
- Możesz użyć zdalnego adresu IP jeśli Loki jest na innym serwerze

#### Sekcja `scrape_configs` - konfiguracja zbierania

```yaml
- job_name: audit
```

**job_name**: Nazwa zadania - stanie się etykietą. Używana do identyfikacji źródła logów.

```yaml
labels:
  job: audit
  source: auditd
  __path__: /var/log/audit/audit.log
```

**labels** - etykiety (labels) dodawane do każdego logu:
- `job: audit` - główna etykieta identyfikująca job
- `source: auditd` - dodatkowa etykieta oznaczająca źródło
- `__path__: /var/log/audit/audit.log` - **KLUCZOWE** - ścieżka do pliku logów

**Uwaga o `__path__`:**
- Specjalna etykieta zaczynająca się od `__` (dwa podkreślenia)
- NIE jest wysyłana do Lokiego jako etykieta
- Określa jakie pliki Promtail ma monitorować
- Obsługuje wildcards: `/var/log/audit/*.log`, `/var/log/**/*.log`

**Przykład zastosowania etykiet w zapytaniach LogQL:**
```logql
{job="audit"}                      # Wszystkie logi z job=audit
{job="audit", source="auditd"}     # Bardziej precyzyjne filtrowanie
```

#### Sekcja `pipeline_stages` - przetwarzanie logów

Pipeline to sekwencja operacji wykonywanych na każdej linii logu przed wysłaniem do Lokiego.

**Stage 1: Regex - parsowanie logów**

```yaml
- regex:
    expression: 'type=(?P<audit_type>\w+).*msg=audit\((?P<timestamp>[^)]+)\).*key="(?P<audit_key>[^"]+)"'
```

**Przykładowy log auditd:**
```
type=EXECVE msg=audit(1707825600.123:456): argc=2 a0="sudo" a1="whoami" key="sudo_execution"
type=SYSCALL msg=audit(1707825600.124:457): arch=c000003e syscall=59 success=yes key="exec_commands"
```

**Regex wyciąga:**
- `(?P<audit_type>\w+)` → **audit_type** = "EXECVE" lub "SYSCALL"
- `(?P<timestamp>[^)]+)` → **timestamp** = "1707825600.123:456"
- `(?P<audit_key>[^"]+)` → **audit_key** = "sudo_execution" lub "exec_commands"

**Przykłady wartości:**
- `audit_type="EXECVE"` - wykonanie programu
- `audit_type="SYSCALL"` - wywołanie systemowe
- `audit_type="USER_AUTH"` - autentykacja użytkownika
- `audit_key="sudo_execution"` - klucz zdefiniowany w regułach auditd

**Stage 2: Labels - tworzenie etykiet**

```yaml
- labels:
    audit_type:
    audit_key:
```

Wyparsowane wartości stają się **etykietami** w Loki:
- Oryginalny log: `type=EXECVE ... key="sudo_execution"`
- Po przetworzeniu w Loki: `{job="audit", source="auditd", audit_type="EXECVE", audit_key="sudo_execution"}`

**Dlaczego to ważne:**
Loki indeksuje **tylko etykiety**, nie treść logów. Dzięki temu możesz robić szybkie zapytania:

```logql
# Wszystkie zdarzenia EXECVE
{job="audit", audit_type="EXECVE"}

# Tylko użycie sudo
{job="audit", audit_key="sudo_execution"}

# Liczba zdarzeń na minutę
sum(rate({job="audit", audit_type="EXECVE"}[1m]))
```

### Utworzenie usługi systemd dla Promtail

```bash
sudo nano /etc/systemd/system/promtail.service
```

**Zawartość pliku usługi:**

```ini
[Unit]
Description=Promtail Log Agent
Documentation=https://grafana.com/docs/loki/latest/clients/promtail/
After=network.target auditd.service
Wants=network-online.target

[Service]
Type=simple
User=root
Group=root
ExecStart=/usr/local/bin/promtail -config.file=/etc/promtail/promtail-config.yaml
Restart=on-failure
RestartSec=10

# Logi do journald
StandardOutput=journal
StandardError=journal
SyslogIdentifier=promtail

[Install]
WantedBy=multi-user.target
```

**Wyjaśnienie opcji:**
- **After=auditd.service**: Uruchamia Promtail po auditd
- **User=root**: Wymagane dla dostępu do `/var/log/audit/audit.log`
- **Restart=on-failure**: Automatyczny restart po błędzie

### Uruchomienie Promtail

```bash
# Przeładowanie konfiguracji systemd
sudo systemctl daemon-reload

# Włączenie autostartu
sudo systemctl enable promtail

# Uruchomienie usługi
sudo systemctl start promtail

# Sprawdzenie statusu
sudo systemctl status promtail
```

### Weryfikacja działania

#### 1. Sprawdzenie statusu Promtail

```bash
# Status usługi
sudo systemctl status promtail

# Logi z ostatniej minuty
sudo journalctl -u promtail -n 50

# Logi w czasie rzeczywistym
sudo journalctl -u promtail -f
```

#### 2. Sprawdzenie metryk Promtail

```bash
# Health check
curl http://localhost:9080/ready

# Metryki Prometheusa
curl http://localhost:9080/metrics | grep promtail

# Liczba wysłanych logów
curl -s http://localhost:9080/metrics | grep promtail_sent_entries_total
```

#### 3. Sprawdzenie czy Loki dostał logi

```bash
# Lista dostępnych jobs w Loki
curl -s "http://localhost:3100/loki/api/v1/label/job/values"
# Powinno zwrócić: ["audit"]

# Lista typów auditd
curl -s "http://localhost:3100/loki/api/v1/label/audit_type/values"
# Powinno zwrócić: ["EXECVE", "SYSCALL", "USER_AUTH", ...]

# Ostatnie 10 logów
curl -G "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query={job="audit"}' \
  --data-urlencode 'limit=10'
```

#### 4. Test w Grafanie

W **Grafana → Explore**:

```logql
# Wszystkie logi auditd
{job="audit"}

# Tylko wykonane polecenia
{job="audit", audit_type="EXECVE"}

# Użycie sudo
{job="audit", audit_key="sudo_execution"}

# Liczba zdarzeń na minutę (wykres)
sum(rate({job="audit"}[1m]))

# Top 10 typów zdarzeń
topk(10, sum by (audit_type) (count_over_time({job="audit"}[5m])))
```

### Rozwiązywanie problemów

#### Problem 1: Promtail nie widzi pliku audit.log

```bash
# Sprawdź czy plik istnieje
ls -la /var/log/audit/audit.log

# Sprawdź uprawnienia
# Promtail (uruchomiony jako root) musi mieć dostęp do odczytu

# Sprawdź czy auditd działa
sudo systemctl status auditd

# Sprawdź grupę
sudo usermod -a -G adm root
```

#### Problem 2: Loki nie dostaje logów

```bash
# Sprawdź logi Promtail
sudo journalctl -u promtail -n 100

# Sprawdź czy Loki nasłuchuje
curl http://localhost:3100/ready

# Sprawdź connectivity
telnet localhost 3100
```

#### Problem 3: Regex nie parsuje logów

```bash
# Wyświetl surowe logi auditd
sudo tail -5 /var/log/audit/audit.log

# Testuj regex na https://regex101.com/
# Pattern: type=(?P<audit_type>\w+).*msg=audit\((?P<timestamp>[^)]+)\).*key="(?P<audit_key>[^"]+)"
# Próbka: type=EXECVE msg=audit(1707825600.123:456): argc=2 key="sudo_execution"
```

#### Problem 4: Zbyt dużo etykiet (high cardinality)

**Uwaga:** Nie dodawaj jako etykiet wartości o dużej zmienności (high cardinality):
- ❌ Timestamp (millisekundy)
- ❌ Process ID (PID)
- ❌ Pełna komenda z argumentami
- ✅ Typ zdarzenia (audit_type)
- ✅ Nazwa klucza (audit_key)
- ✅ Poziom logowania

**Dlaczego?** Loki tworzy osobny indeks dla każdej unikalnej kombinacji etykiet. Zbyt wiele unikalnych wartości spowalnia Loki.

### Zaawansowana konfiguracja pipeline

Bardziej rozbudowany pipeline z dodatkowymi funkcjami:

```yaml
pipeline_stages:
  # Stage 1: Wyciągnij podstawowe pola
  - regex:
      expression: 'type=(?P<audit_type>\w+).*msg=audit\((?P<timestamp>[0-9.]+):(?P<audit_id>\d+)\)'
  
  # Stage 2: Dodaj jako etykiety
  - labels:
      audit_type:
  
  # Stage 3: Wyciągnij szczegóły dla EXECVE
  - match:
      selector: '{audit_type="EXECVE"}'
      stages:
        - regex:
            expression: 'a0="(?P<command>[^"]+)"'
        - output:
            source: command
  
  # Stage 4: Wyciągnij użytkownika (uid)
  - regex:
      expression: 'uid=(?P<uid>\d+)'
  
  # Stage 5: Wyciągnij klucz auditd
  - regex:
      expression: 'key="(?P<key>[^"]+)"'
  
  # Stage 6: Dodaj key jako etykietę
  - labels:
      key:
  
  # Stage 7: Drop nieistotnych logów (opcjonalnie)
  - drop:
      expression: 'type=(PROCTITLE|CWD|PATH)'
      drop_counter_reason: "filtered_noise"
```

### Przykładowe zapytania LogQL dla auditd

Po skonfigurowaniu Promtail, w Grafanie możesz używać następujących zapytań:

#### Podstawowe zapytania:

```logql
# Wszystkie logi auditd
{job="audit"}

# Tylko EXECVE (wykonane komendy)
{job="audit", audit_type="EXECVE"}

# Zmiany w /etc/passwd
{job="audit", audit_key="passwd_changes"}

# Użycie sudo
{job="audit", audit_key="sudo_execution"}
```

#### Zapytania analityczne (dla wykresów):

```logql
# Liczba zdarzeń na minutę
sum(rate({job="audit"}[1m]))

# Top 10 typów zdarzeń
topk(10, sum by (audit_type) (count_over_time({job="audit"}[1h])))

# Alerty - zmiany w plikach krytycznych
count_over_time({job="audit", audit_key="passwd_changes"}[5m]) > 0

# Porównanie aktywności w czasie
sum by (audit_type) (rate({job="audit"}[5m]))
```

### Integracja z dashboardem Grafana

Przykładowe panele do utworzenia w Grafanie:

**Panel 1: Liczba zdarzeń audytu (Time Series)**
- Query: `sum(rate({job="audit"}[1m]))`
- Type: Time series
- Title: "Aktywność audytu - zdarzenia/s"

**Panel 2: Top typów zdarzeń (Bar Chart)**
- Query: `topk(15, sum by (audit_type) (count_over_time({job="audit"}[1h])))`
- Type: Bar chart
- Title: "Top 15 typów zdarzeń auditd"

**Panel 3: Surowe logi (Logs)**
- Query: `{job="audit"}`
- Type: Logs
- Title: "Logi auditd w czasie rzeczywistym"

**Panel 4: Alerty bezpieczeństwa (Stat)**
- Query: `count_over_time({job="audit", audit_key=~"passwd_changes|shadow_changes|sudoers_changes"}[5m])`
- Type: Stat
- Title: "Zmiany w plikach krytycznych (5 min)"
- Threshold: > 0 = czerwony

---

## Praktyczne używanie labeli w zapytaniach LogQL

Ta sekcja szczegółowo wyjaśnia jak używać etykiet (labels) w LogQL na praktycznych przykładach z logów auditd generowanych przez skrypt automatyczny.

### Czym są labels (etykiety)?

**Labels** to pary klucz-wartość przypisane do każdego logu przez Promtail:

```
{job="audit", source="auditd", audit_type="EXECVE", audit_key="sudo_execution"}
```

**Kluczowa zasada Lokiego:** Loki indeksuje **TYLKO etykiety**, nie treść logów. To sprawia, że zapytania po etykietach są błyskawiczne, nawet przy miliardach logów.

### Etykiety dostępne w konfiguracji auditd

Z naszej konfiguracji Promtail dla auditd mamy następujące etykiety:

| Etykieta | Źródło | Przykładowe wartości |
|----------|--------|---------------------|
| `job` | Statyczne (config) | `audit` |
| `source` | Statyczne (config) | `auditd` |
| `audit_type` | Dynamiczne (regex) | `EXECVE`, `SYSCALL`, `USER_AUTH`, `PATH`, `PROCTITLE` |
| `audit_key` | Dynamiczne (regex) | `passwd_changes`, `sudo_execution`, `tmp_directory`, `exec_commands` |

### Operatory dla labeli (Label Matchers)

LogQL obsługuje 4 operatory do filtrowania po etykietach:

#### 1. `=` – Równe (Equal)

Zwraca logi gdzie etykieta **dokładnie odpowiada** wartości.

```logql
{job="audit"}
```

**Co to zwraca:** Wszystkie logi gdzie `job` jest **dokładnie** równe `audit`.

**Przykład z auditd:**
```logql
{audit_type="EXECVE"}
```
Zwraca tylko zdarzenia typu EXECVE (wykonanie programu), pomija SYSCALL, USER_AUTH, itp.

#### 2. `!=` – Różne (Not Equal)

Zwraca logi gdzie etykieta **nie jest równa** wartości.

```logql
{audit_type!="PROCTITLE"}
```

**Co to zwraca:** Wszystkie logi auditd **z wyjątkiem** typu PROCTITLE.

**Przykład praktyczny:**
```logql
{job="audit", audit_type!="CWD"}
```
Wszystkie zdarzenia auditd oprócz CWD (change working directory), które często są szumem informacyjnym.

#### 3. `=~` – Regex Match

Zwraca logi gdzie etykieta **pasuje do wyrażenia regularnego**.

```logql
{audit_key=~"passwd_changes|shadow_changes"}
```

**Co to zwraca:** Logi gdzie `audit_key` to **passwd_changes** ALBO **shadow_changes**.

**Przykład praktyczny z auditd:**
```logql
{job="audit", audit_key=~".*_changes"}
```
Wszystkie zdarzenia dotyczące zmian w plikach (passwd_changes, group_changes, shadow_changes, sudoers_changes, sshd_config_changes).

**Zaawansowane regex:**
```logql
{audit_key=~"sudo_.*|ssh_.*"}
```
Wszystkie zdarzenia związane z sudo (sudo_execution) lub ssh (ssh_execution).

#### 4. `!~` – Regex Not Match

Zwraca logi gdzie etykieta **nie pasuje** do wyrażenia regularnego.

```logql
{audit_type!~"PROCTITLE|CWD|PATH"}
```

**Co to zwraca:** Wszystkie logi auditd **z wyjątkiem** typów PROCTITLE, CWD, PATH.

**Przykład praktyczny:**
```logql
{job="audit", audit_type!~"PROCTITLE|CWD"}
```
Filtruje szum informacyjny, zostawiając tylko istotne zdarzenia.

### Łączenie wielu labeli

Można łączyć wiele warunków w jednym selektorze. **Wszystkie warunki muszą być spełnione** (logiczne AND).

#### Przykład 1: Dokładne dopasowanie dwóch labeli

```logql
{job="audit", audit_type="EXECVE"}
```

**Wynik:** Tylko logi z `job="audit"` **ORAZ** `audit_type="EXECVE"`.

#### Przykład 2: Kombinacja różnych operatorów

```logql
{job="audit", audit_type="SYSCALL", audit_key=~"sudo_.*"}
```

**Wynik:** Tylko zdarzenia systemowe (SYSCALL) związane z sudo.

#### Przykład 3: Wykluczanie szumu

```logql
{job="audit", audit_type!~"PROCTITLE|CWD|PATH", audit_key!=""}
```

**Wynik:** Istotne zdarzenia auditd, bez szumu informacyjnego, tylko te z przypisanym kluczem.

### Różnica: Label Selector vs Line Filter

To **kluczowa koncepcja** w LogQL!

#### Label Selector (selektor etykiet)

```logql
{job="audit", audit_type="EXECVE"}
```

- Filtrowanie po **etykietach** (indeksowanych)
- **Bardzo szybkie** (milisekundy)
- Działa na poziomie indeksu
- Zwraca **strumienie** logów

#### Line Filter (filtr linii)

```logql
{job="audit"} |= "sudo"
```

- Filtrowanie po **treści** logów
- **Wolniejsze** (skanuje treść)
- Działa na poziomie zawartości
- Operator: `|=` (zawiera), `!=` (nie zawiera), `|~` (regex), `!~` (not regex)

#### Kombinacja (zalecana dla wydajności)

```logql
{job="audit", audit_type="EXECVE"} |= "sudo"
```

**Kolejność wykonania:**
1. **Szybko:** Znajdź wszystkie logi z `audit_type="EXECVE"` (index lookup)
2. **Następnie:** W tych logach szukaj słowa "sudo" (content scan)

**Zawsze najpierw filtruj po labelach, potem po treści!**

### Praktyczne przykłady z auditd

#### Przykład 1: Monitoring dostępu do plików systemowych

**Zadanie:** Wyświetl wszystkie dostępy do plików krytycznych (/etc/passwd, /etc/shadow, /etc/sudoers).

```logql
{job="audit", audit_key=~"passwd_changes|shadow_changes|sudoers_changes"}
```

**Wyjaśnienie:**
- `job="audit"` - tylko logi auditd
- `audit_key=~"..."` - regex dopasowanie do trzech kluczy naraz

**Użycie w dashboardzie:**
- Panel typu **Logs** - zobacz kto i kiedy modyfikował pliki
- Panel typu **Stat** z `count_over_time()` - liczba zmian w ostatnich 5 minutach
- Alert jeśli liczba > 0

#### Przykład 2: Monitoring wykonywanych poleceń

**Zadanie:** Wyświetl wszystkie wykonane polecenia (EXECVE).

```logql
{job="audit", audit_type="EXECVE"}
```

**Generator auditd wywołuje:** whoami, id, hostname, uptime, df, free, ps

**Wynik:** Zobaczysz wszystkie te polecenia w czasie rzeczywistym.

**Rozwinięcie - tylko sudo:**
```logql
{job="audit", audit_type="EXECVE", audit_key="sudo_execution"}
```

Tylko polecenia wykonane przez sudo.

#### Przykład 3: Filtrowanie po treści + etykiety

**Zadanie:** Znajdź wszystkie zdarzenia EXECVE zawierające słowo "passwd".

```logql
{job="audit", audit_type="EXECVE"} |= "passwd"
```

**Wyjaśnienie:**
1. `{job="audit", audit_type="EXECVE"}` - label selector (szybki)
2. `|= "passwd"` - line filter (przeszukuje treść)

**Przykład praktyczny z generatora:**
Generator auditd często wywołuje dostęp do `/etc/passwd`. To zapytanie pokaże te zdarzenia.

#### Przykład 4: Wykluczanie szumu

**Zadanie:** Wyświetl wszystkie ważne zdarzenia, pomijając szum informacyjny.

```logql
{job="audit", audit_type!~"PROCTITLE|CWD|PATH"}
```

**Wyjaśnienie:**
- PROCTITLE, CWD, PATH to pomocnicze typy zdarzeń
- Często są generowane ale mają małą wartość informacyjną
- Wykluczamy je regexem `!~`

#### Przykład 5: Monitoring połączeń sieciowych

**Zadanie:** Wyświetl tylko zdarzenia związane z połączeniami sieciowymi.

```logql
{job="audit", audit_key="network_connections"}
```

**Generator auditd** próbuje łączyć się z: google.com, localhost, 127.0.0.1

**Wynik:** Zobaczysz te próby połączeń w logach.

#### Przykład 6: Aktywność w katalogu /tmp

**Zadanie:** Monitoruj wszystkie operacje na plikach w /tmp.

```logql
{job="audit", audit_key="tmp_directory"}
```

**Generator auditd** tworzy pliki w /tmp, modyfikuje je i usuwa.

**Wynik:** Lista wszystkich operacji na `/tmp/audit_test_*.tmp`.

### Agregacje i funkcje z labelami

Po wybraniu strumieni przez label selector, możesz agregować dane:

#### count_over_time() - Liczba logów w przedziale czasowym

```logql
count_over_time({job="audit", audit_type="EXECVE"}[5m])
```

**Co to robi:** Liczy ile było zdarzeń EXECVE w ostatnich 5 minutach.

**Użycie:** Panel typu **Time series** pokazujący trend wykonywanych poleceń.

#### rate() - Tempo zmian

```logql
rate({job="audit", audit_key="sudo_execution"}[1m])
```

**Co to robi:** Oblicza tempo używania sudo (zdarzenia/sekundę) w ostatniej minucie.

**Użycie:** Wykrycie podejrzanej aktywności (spike w użyciu sudo).

#### sum by() - Agregacja po etykietach

```logql
sum by (audit_type) (count_over_time({job="audit"}[5m]))
```

**Co to robi:** 
1. Liczy zdarzenia w ostatnich 5 minutach
2. Grupuje po `audit_type`
3. Sumuje dla każdego typu

**Wynik:** 
```
{audit_type="EXECVE"}: 150
{audit_type="SYSCALL"}: 300
{audit_type="USER_AUTH"}: 10
```

**Użycie:** Panel **Bar chart** pokazujący rozkład typów zdarzeń.

#### topk() - Top N wartości

```logql
topk(5, sum by (audit_key) (count_over_time({job="audit"}[1h])))
```

**Co to robi:** Pokazuje TOP 5 najczęściej występujących kluczy auditd w ostatniej godzinie.

**Przykładowy wynik z generatora:**
```
1. exec_commands: 450
2. sudo_execution: 120
3. tmp_directory: 80
4. passwd_changes: 45
5. network_connections: 30
```

**Użycie:** Panel **Bar chart** - "Top 5 aktywności audytu".

### Praktyczne scenariusze z generatorem auditd

#### Scenariusz 1: Dashboard "Przegląd bezpieczeństwa"

**Panel 1: Ogólna aktywność**
```logql
sum(rate({job="audit"}[1m]))
```
Wykres linii pokazujący ~10 zdarzeń/minutę (generator co 30s generuje 5 zdarzeń).

**Panel 2: Rozkład typów zdarzeń**
```logql
sum by (audit_type) (count_over_time({job="audit"}[5m]))
```
Widać proporcje: EXECVE, SYSCALL, PATH, itp.

**Panel 3: Top aktywności**
```logql
topk(10, sum by (audit_key) (count_over_time({job="audit"}[1h])))
```
Ranking: exec_commands, sudo_execution, tmp_directory, passwd_changes, etc.

**Panel 4: Krytyczne zdarzenia**
```logql
count_over_time({job="audit", audit_key=~"passwd_changes|shadow_changes|sudoers_changes"}[5m])
```
Alert: czerwony jeśli > 0 (ktoś modyfikuje pliki systemowe).

#### Scenariusz 2: "Kto i co wykonuje?"

**Zapytanie:**
```logql
{job="audit", audit_type="EXECVE"} | regexp "a0=\"(?P<command>[^\"]+)\""
```

**Wyjaśnienie:**
1. Wybierz zdarzenia EXECVE
2. Wyciągnij nazwę komendy regexem
3. Wyświetl jako kolumnę

**Wynik w Grafanie (panel Logs):**
```
timestamp                command
2026-02-13 16:30:01     whoami
2026-02-13 16:30:15     hostname
2026-02-13 16:30:32     uptime
2026-02-13 16:30:48     sudo
2026-02-13 16:31:05     ps aux
```

#### Scenariusz 3: Alert "Podejrzana aktywność sudo"

**Warunek:** Więcej niż 10 użyć sudo w ciągu 5 minut.

**Zapytanie:**
```logql
count_over_time({job="audit", audit_key="sudo_execution"}[5m]) > 10
```

**Konfiguracja w Grafanie:**
- Type: Stat
- Threshold: > 10 = czerwony
- Alert rule: Wyślij powiadomienie na email/Slack

**Normalny poziom z generatora:** ~3-5 użyć sudo / 5 min
**Alert:** Jeśli przekroczy 10 = podejrzana eskalacja uprawnień

#### Scenariusz 4: Monitoring dostępów do plików SSH

**Zapytanie:**
```logql
{job="audit", audit_key="sshd_config_changes"}
```

**Generator auditd** okresowo wykonuje `cat /etc/ssh/sshd_config`.

**Dashboard:**
- Panel **Logs**: Zobacz dokładnie kiedy ktoś czytał/modyfikował sshd_config
- Panel **Stat**: Liczba dostępów w ostatniej godzinie
- Alert: Jeśli > 0 modyfikacji (write access)

#### Scenariusz 5: Wykres aktywności w czasie

**Porównanie różnych typów aktywności:**

```logql
sum by (audit_key) (rate({job="audit", audit_key=~"sudo_execution|passwd_changes|tmp_directory"}[1m]))
```

**Wynik:** Wykres z trzema liniami:
- 🔵 sudo_execution - stała, ~2-3 zdarzenia/min
- 🟢 tmp_directory - spike'i gdy generator tworzy pliki
- 🔴 passwd_changes - spike'i gdy generator czyta /etc/passwd

### Best Practices - dobre praktyki

#### ✅ DO (Rób tak):

1. **Najpierw label selector, potem line filter**
```logql
# ✅ Dobrze
{job="audit", audit_type="EXECVE"} |= "sudo"

# ❌ Źle (wolniejsze)
{job="audit"} |= "sudo" | regexp "type=EXECVE"
```

2. **Używaj konkretnych labeli zamiast szukać w treści**
```logql
# ✅ Dobrze
{audit_key="sudo_execution"}

# ❌ Źle (wolniejsze)
{job="audit"} |= "sudo"
```

3. **Agreguj po labelach, nie po treści**
```logql
# ✅ Dobrze
sum by (audit_type) (count_over_time({job="audit"}[5m]))

# ❌ Możliwe ale wolniejsze
{job="audit"} | regexp "type=(?P<type>\w+)" | unwrap type | sum by (type)
```

#### ❌ DON'T (Unikaj):

1. **Nie twórz labeli o wysokiej zmienności (high cardinality)**
```yaml
# ❌ ŹLE - każdy timestamp to unikalna etykieta
- labels:
    timestamp:  # Miliony unikalnych wartości!

# ✅ DOBRZE - ograniczona liczba wartości
- labels:
    audit_type:  # ~10 wartości
    audit_key:   # ~20 wartości
```

2. **Nie używaj regex tam gdzie wystarczy `=`**
```logql
# ❌ Wolniejsze
{audit_type=~"EXECVE"}

# ✅ Szybsze
{audit_type="EXECVE"}
```

3. **Nie pomijaj label selector**
```logql
# ❌ Skanuje WSZYSTKIE logi
|= "error"

# ✅ Najpierw filtr po etykietach
{job="audit"} |= "error"
```

### Debugowanie zapytań

#### Problem: "Zapytanie nic nie zwraca"

**Krok 1:** Sprawdź dostępne etykiety
```bash
curl -s "http://localhost:3100/loki/api/v1/labels"
```

**Krok 2:** Sprawdź wartości dla konkretnej etykiety
```bash
curl -s "http://localhost:3100/loki/api/v1/label/audit_type/values"
```

**Krok 3:** Uproszczone zapytanie
```logql
# Zamiast skomplikowanego zapytania, zacznij od prostego
{job="audit"}
```

**Krok 4:** Stopniowo dodawaj warunki
```logql
{job="audit", audit_type="EXECVE"}
{job="audit", audit_type="EXECVE"} |= "sudo"
```

#### Problem: "Zapytanie jest wolne"

**Diagnoza:**
1. Czy używasz label selector jako pierwszego?
2. Czy przedział czasowy nie jest za duży? (np. `[24h]` może być powolne)
3. Czy nie masz zbyt wielu unikalnych labeli?

**Optymalizacja:**
```logql
# ❌ Wolne (szeroki zakres bez filtrów)
{job="audit"}[24h]

# ✅ Szybsze (konkretny typ i krótszy zakres)
{job="audit", audit_type="EXECVE"}[1h]

# ✅ Jeszcze szybsze (agregacja zamiast raw logs)
count_over_time({job="audit", audit_type="EXECVE"}[1h])
```

### Szybka ściągawka - operatory

| Operator | Nazwa | Przykład | Wynik |
|----------|-------|----------|-------|
| `=` | Równe | `{job="audit"}` | Dokładne dopasowanie |
| `!=` | Różne | `{audit_type!="PROCTITLE"}` | Wszystko oprócz PROCTITLE |
| `=~` | Regex match | `{audit_key=~".*_changes"}` | Wszystkie kończące się na _changes |
| `!~` | Regex not match | `{audit_type!~"PROCTITLE\|CWD"}` | Wszystko oprócz PROCTITLE i CWD |
| `\|=` | Zawiera tekst | `{job="audit"} \|= "sudo"` | Logi zawierające "sudo" |
| `!=` | Nie zawiera | `{job="audit"} != "debug"` | Logi bez słowa "debug" |
| `\|~` | Regex w treści | `{job="audit"} \|~ "error\|warning"` | Logi z error lub warning |
| `!~` | Not regex treści | `{job="audit"} !~ "debug"` | Logi bez słowa debug |

### Podsumowanie

**Kluczowe zasady:**
1. **Labels są szybkie** - użyj ich do wstępnego filtrowania
2. **Line filters są wolniejsze** - użyj po label selector
3. **Kombinuj operatory** - twórz precyzyjne zapytania
4. **Agreguj inteligentnie** - używaj `sum by()`, `topk()`, `rate()`
5. **Unikaj high cardinality** - nie twórz labeli z unikalnych wartości

**Z generatorem auditd masz:**
- ~300 zdarzeń/godzinę (5 zdarzeń co 30s)
- 4-5 różnych wartości `audit_type`
- 10-15 różnych wartości `audit_key`
- Idealne środowisko do nauki LogQL!

---

## Zabezpieczenie instalacji

### 1. Konfiguracja firewalla (UFW)

Ograniczenie dostępu do Lokiego za pomocą firewalla:

```bash
# Włączenie UFW
sudo ufw enable

# Zezwolenie na port 3100 tylko z określonej sieci
sudo ufw allow from 10.0.0.0/24 to any port 3100

# Zezwolenie na port 9096 dla agentów Promtail
sudo ufw allow from 10.0.0.0/24 to any port 9096

# Sprawdzenie reguł
sudo ufw status numbered

# Zezwolenie na SSH (jeśli jeszcze nie dodano)
sudo ufw allow 22/tcp
```

**Wyjaśnienie:**

Reguły firewalla ograniczają dostęp do Lokiego tylko z określonej sieci (np. 10.0.0.0/24).

### 2. Konfiguracja reverse proxy (nginx)

Zabezpieczenie dostępu do Lokiego przez nginx:

```bash
# Instalacja nginx
sudo apt install -y nginx

# Utworzenie pliku konfiguracyjnego
sudo nano /etc/nginx/sites-available/loki
```

Zawartość pliku konfiguracyjnego nginx:

```nginx
server {
    listen 80;
    server_name loki.example.com;

    # Ograniczenie dostępu do określonych IP
    allow 10.0.0.0/24;
    deny all;

    location / {
        proxy_pass http://localhost:3100;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

Aktywacja konfiguracji:

```bash
# Włączenie konfiguracji
sudo ln -s /etc/nginx/sites-available/loki /etc/nginx/sites-enabled/

# Sprawdzenie składni
sudo nginx -t

# Przeładowanie nginx
sudo systemctl reload nginx
```

### 3. Konfiguracja SSL/TLS z Let's Encrypt

Zabezpieczenie połączenia SSL/TLS:

```bash
# Instalacja certbot
sudo apt install -y certbot python3-certbot-nginx

# Uzyskanie certyfikatu SSL
sudo certbot --nginx -d loki.example.com

# Automatyczne odnowienie certyfikatu (certbot dodaje cronjob automatycznie)
sudo certbot renew --dry-run
```

Certbot automatycznie modyfikuje konfigurację nginx, dodając sekcję SSL:

```nginx
server {
    listen 443 ssl;
    server_name loki.example.com;

    ssl_certificate /etc/letsencrypt/live/loki.example.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/loki.example.com/privkey.pem;

    location / {
        proxy_pass http://localhost:3100;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

---

## Troubleshooting

### Problem 1: Loki nie startuje

Diagnostyka problemu:

```bash
# Sprawdzenie logów systemd
sudo journalctl -u loki -n 100 --no-pager

# Sprawdzenie uprawnień do katalogów
ls -la /var/lib/loki
ls -la /etc/loki

# Weryfikacja składni pliku konfiguracyjnego
loki -config.file=/etc/loki/loki-config.yaml -verify-config

# Sprawdzenie czy port nie jest zajęty
sudo lsof -i :3100
```

Typowe przyczyny:
- Błędna składnia w pliku konfiguracyjnym
- Brak uprawnień do katalogów
- Zajęty port 3100

### Problem 2: Port już zajęty

Identyfikacja procesu zajmującego port:

```bash
# Sprawdzenie procesu używającego portu 3100
sudo lsof -i :3100

# Alternatywnie z użyciem ss
sudo ss -tlnp | grep 3100

# Zatrzymanie procesu zajmującego port
sudo kill -9 <PID>

# Lub zmiana portu w konfiguracji Lokiego
```

### Problem 3: Brak miejsca na dysku

Analiza wykorzystania dysku:

```bash
# Sprawdzenie użycia dysku
df -h /var/lib/loki

# Sprawdzenie rozmiaru katalogów Lokiego
du -sh /var/lib/loki/*

# Szczegółowa analiza rozmiaru
du -h /var/lib/loki | sort -h

# Sprawdzenie liczby plików
find /var/lib/loki -type f | wc -l
```

Rozwiązania:
- Konfiguracja retencji (automatyczne usuwanie starych logów)
- Zwiększenie przestrzeni dyskowej
- Przeniesienie danych na większy dysk
- Użycie zewnętrznego storage (S3)

### Problem 4: Wysoka latencja zapytań

Diagnoza wydajności:

```bash
# Sprawdzenie metryk wydajności
curl http://localhost:3100/metrics | grep loki_ingester

# Sprawdzenie metryk zapytań
curl http://localhost:3100/metrics | grep loki_query_duration

# Sprawdzenie liczby strumieni
curl http://localhost:3100/metrics | grep loki_ingester_streams

# Monitorowanie zasobów systemowych
top
htop
```

Rozwiązania:
- Zwiększenie zasobów (RAM, CPU)
- Optymalizacja zapytań LogQL
- Zmniejszenie liczby etykiet (kardynalności)
- Konfiguracja cache

### Problem 5: Błędy ingestion rate limit

Komunikat błędu:

```
ingestion rate limit exceeded for user (limit: 10MB/sec) while attempting to ingest '15' lines totaling '2MB'
```

Rozwiązanie:

```yaml
# Zwiększenie limitów w konfiguracji
limits_config:
  ingestion_rate_mb: 50
  ingestion_burst_size_mb: 100
```

```bash
# Restart Lokiego po zmianie konfiguracji
sudo systemctl restart loki
```

---

## Testowanie instalacji

### Test 1: Wysłanie testowego logu przez API

Wysłanie logu do Lokiego za pomocą curl:

```bash
# Wysłanie testowego logu
curl -X POST http://localhost:3100/loki/api/v1/push \
  -H "Content-Type: application/json" \
  -d '{
    "streams": [
      {
        "stream": {
          "job": "test",
          "host": "localhost"
        },
        "values": [
          ["'$(date +%s)000000000'", "Test log message from curl"]
        ]
      }
    ]
  }'
```

**Wyjaśnienie formatu:**

- **streams**: Tablica strumieni logów
- **stream**: Etykiety identyfikujące strumień (job, host)
- **values**: Tablica zawierająca pary [timestamp, message]
- Timestamp w formacie nanosekund Unix epoch

### Test 2: Zapytanie o logi

Pobranie logów za pomocą API:

```bash
# Zapytanie LogQL przez API
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query={job="test"}' \
  --data-urlencode 'limit=10' | jq
```

**Uwaga:** Wymaga zainstalowanego `jq`: `sudo apt install jq`

### Test 3: Zapytanie range query

Zapytanie o logi z określonego przedziału czasowego:

```bash
# Range query - ostatnie 5 minut
curl -G -s "http://localhost:3100/loki/api/v1/query_range" \
  --data-urlencode 'query={job="test"}' \
  --data-urlencode 'start='$(date -d '5 minutes ago' +%s)000000000 \
  --data-urlencode 'end='$(date +%s)000000000 \
  --data-urlencode 'limit=100' | jq
```

### Test 4: Sprawdzenie metryk

Analiza metryk Lokiego:

```bash
# Metryki ingester
curl -s http://localhost:3100/metrics | grep loki_ingester_streams_created_total

# Metryki zapytań
curl -s http://localhost:3100/metrics | grep loki_query_duration_seconds

# Metryki otrzymanych logów
curl -s http://localhost:3100/metrics | grep loki_distributor_lines_received_total
```

### Test 5: Sprawdzenie statusu komponentów

```bash
# Status wszystkich komponentów
curl -s http://localhost:3100/services | jq

# Lista labelów
curl -s http://localhost:3100/loki/api/v1/labels | jq

# Wartości konkretnej etykiety
curl -s http://localhost:3100/loki/api/v1/label/job/values | jq
```

---

## Monitorowanie Lokiego

### Metryki Prometheus

Loki eksponuje metryki w formacie Prometheus na endpoint `/metrics`.

Kluczowe metryki do monitorowania:

| Metryka | Opis |
|---------|------|
| `loki_ingester_streams_created_total` | Liczba utworzonych strumieni |
| `loki_distributor_lines_received_total` | Liczba otrzymanych linii logów |
| `loki_distributor_bytes_received_total` | Liczba otrzymanych bajtów |
| `loki_ingester_chunks_stored_total` | Liczba przechowywanych chunków |
| `loki_query_duration_seconds` | Czas wykonywania zapytań |
| `loki_request_duration_seconds` | Czas przetwarzania żądań HTTP |

### Konfiguracja Prometheusa dla Lokiego

Fragment konfiguracji `prometheus.yml`:

```yaml
scrape_configs:
  - job_name: 'loki'
    static_configs:
      - targets: ['localhost:3100']
    scrape_interval: 15s
```

---

## Podsumowanie

Instalacja i konfiguracja Lokiego została omówiona w dwóch wariantach:
- Instalacja z binarki jako usługa systemd
- Instalacja w kontenerze Docker

**Kluczowe lokalizacje:**

| Lokalizacja | Opis |
|-------------|------|
| `/usr/local/bin/loki` | Binarka Lokiego |
| `/etc/loki/loki-config.yaml` | Plik konfiguracyjny |
| `/var/lib/loki/` | Katalog z danymi |
| `/etc/systemd/system/loki.service` | Plik usługi systemd |

**Podstawowe komendy:**

```bash
# Zarządzanie usługą
sudo systemctl status loki
sudo systemctl start loki
sudo systemctl stop loki
sudo systemctl restart loki

# Przegląd logów
sudo journalctl -u loki -f

# Weryfikacja działania
curl http://localhost:3100/ready
curl http://localhost:3100/metrics
```

**Porty:**

- **3100** – HTTP API (zapytania, push logów)
- **9096** – gRPC (komunikacja z Promtail)

**Następne kroki:**

- Instalacja i konfiguracja Promtail (agent zbierający logi)
- Integracja z Grafaną
- Tworzenie zapytań LogQL
- Budowa dashboardów
