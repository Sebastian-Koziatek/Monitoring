# ƒÜwiczenie praktyczne - Analiza log√≥w aplikacji webowej

## Cel ƒáwiczenia

Celem ƒáwiczenia jest praktyczne zastosowanie wiedzy o Elasticsearch w kontek≈õcie monitoringu aplikacji webowej. Uczestnicy bƒôdƒÖ indeksowaƒá logi, tworzyƒá zapytania DSL, budowaƒá agregacje oraz wizualizowaƒá dane w Grafanie.

---

## Scenariusz biznesowy

Jeste≈õ odpowiedzialny za monitoring aplikacji e-commerce. Aplikacja sk≈Çada siƒô z kilku mikroserwis√≥w:
- **auth-service** ‚Äì uwierzytelnianie u≈ºytkownik√≥w
- **product-service** ‚Äì katalog produkt√≥w
- **payment-service** ‚Äì obs≈Çuga p≈Çatno≈õci
- **notification-service** ‚Äì wysy≈Çka powiadomie≈Ñ

Twoim zadaniem jest:
1. Zaindeksowaƒá przyk≈Çadowe logi w Elasticsearch
2. Przeprowadziƒá analizƒô b≈Çƒôd√≥w i wydajno≈õci
3. Utworzyƒá dashboard w Grafanie

---

## Czƒô≈õƒá 1: Przygotowanie ≈õrodowiska

### Krok 1: Utworzenie Index Template

Zdefiniuj szablon dla indeks√≥w log√≥w, kt√≥ry automatycznie zastosuje odpowiednie mapowanie.

```json
PUT /_index_template/ecommerce-logs-template
{
  "index_patterns": ["ecommerce-logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 3,
      "number_of_replicas": 1,
      "index.lifecycle.name": "logs-policy",
      "index.lifecycle.rollover_alias": "ecommerce-logs"
    },
    "mappings": {
      "properties": {
        "timestamp": {
          "type": "date",
          "format": "strict_date_optional_time||epoch_millis"
        },
        "level": {
          "type": "keyword"
        },
        "service": {
          "type": "keyword"
        },
        "message": {
          "type": "text",
          "fields": {
            "keyword": {
              "type": "keyword",
              "ignore_above": 256
            }
          }
        },
        "user_id": {
          "type": "keyword"
        },
        "request_id": {
          "type": "keyword"
        },
        "http": {
          "properties": {
            "method": { "type": "keyword" },
            "status_code": { "type": "integer" },
            "endpoint": { "type": "keyword" },
            "user_agent": { "type": "text" }
          }
        },
        "response_time_ms": {
          "type": "integer"
        },
        "error": {
          "properties": {
            "code": { "type": "keyword" },
            "message": { "type": "text" },
            "stack_trace": { "type": "text" }
          }
        },
        "ip_address": {
          "type": "ip"
        },
        "geo": {
          "properties": {
            "country": { "type": "keyword" },
            "city": { "type": "keyword" }
          }
        }
      }
    }
  }
}
```

**Pytania do przemy≈õlenia:**
1. Dlaczego `service` jest typu `keyword`, a nie `text`?
2. Po co u≈ºywamy multi-fields dla `message`?

### Krok 2: Utworzenie pierwszego indeksu

```bash
PUT /ecommerce-logs-2024.02.14
```

---

## Czƒô≈õƒá 2: Zaindeksowanie przyk≈Çadowych danych

### Przygotowanie danych testowych

U≈ºyj poni≈ºszego skryptu Python do wygenerowania i zaindeksowania 1000 przyk≈Çadowych log√≥w:

```python
from elasticsearch import Elasticsearch
from datetime import datetime, timedelta
import random
import json

# Po≈ÇƒÖczenie z Elasticsearch
es = Elasticsearch(['http://localhost:9200'])

# Konfiguracja
services = ['auth-service', 'product-service', 'payment-service', 'notification-service']
levels = ['INFO', 'WARNING', 'ERROR', 'CRITICAL']
level_weights = [70, 20, 8, 2]  # INFO dominuje
endpoints = ['/api/login', '/api/products', '/api/checkout', '/api/notify']
error_codes = ['AUTH_FAILED', 'DB_TIMEOUT', 'PAYMENT_DECLINED', 'NETWORK_ERROR']

# Generowanie 1000 log√≥w
base_time = datetime.now() - timedelta(hours=24)

for i in range(1000):
    # Timestamp co 1-5 minut
    timestamp = base_time + timedelta(minutes=random.randint(1, 5) * i)
    
    # Losowy wyb√≥r serwisu i poziomu
    service = random.choice(services)
    level = random.choices(levels, weights=level_weights)[0]
    
    # Dokument logu
    log = {
        'timestamp': timestamp.isoformat(),
        'level': level,
        'service': service,
        'request_id': f'req-{i:06d}',
        'user_id': f'user-{random.randint(1, 100)}',
        'ip_address': f'192.168.{random.randint(1, 255)}.{random.randint(1, 255)}',
        'http': {
            'method': random.choice(['GET', 'POST', 'PUT', 'DELETE']),
            'status_code': random.choice([200, 201, 400, 401, 404, 500, 502]),
            'endpoint': random.choice(endpoints)
        },
        'response_time_ms': random.randint(10, 5000),
        'geo': {
            'country': random.choice(['PL', 'US', 'DE', 'UK']),
            'city': random.choice(['Warsaw', 'New York', 'Berlin', 'London'])
        }
    }
    
    # Dla b≈Çƒôd√≥w dodaj szczeg√≥≈Çy
    if level in ['ERROR', 'CRITICAL']:
        log['message'] = f'Error in {service}: {random.choice(error_codes)}'
        log['error'] = {
            'code': random.choice(error_codes),
            'message': f'Critical failure in processing request',
            'stack_trace': 'at com.example.Service.process(Service.java:42)'
        }
    else:
        log['message'] = f'{service} processed request successfully'
    
    # Indeksowanie
    index_name = timestamp.strftime('ecommerce-logs-%Y.%m.%d')
    es.index(index=index_name, document=log)
    
    if i % 100 == 0:
        print(f'Indexed {i} documents...')

print('Done! Indexed 1000 documents.')
```

**Alternatywa - bulk API (szybsza):**

```python
from elasticsearch.helpers import bulk

def generate_logs():
    for i in range(1000):
        # ... (jak wy≈ºej)
        yield {
            '_index': index_name,
            '_source': log
        }

bulk(es, generate_logs())
```

### Weryfikacja indeksowania

```bash
# Sprawdzenie liczby dokument√≥w
GET /ecommerce-logs-*/_count

# Przyk≈Çadowe dokumenty
GET /ecommerce-logs-*/_search
{
  "size": 5,
  "sort": [
    { "timestamp": "desc" }
  ]
}
```

**Oczekiwany wynik:** ~1000 dokument√≥w

---

## Czƒô≈õƒá 3: Analiza zapytaniami DSL

### Zadanie 1: Znajd≈∫ wszystkie b≈Çƒôdy krytyczne

**Wymaganie:** Wyszukaj wszystkie logi o poziomie CRITICAL z ostatnich 24 godzin.

```json
GET /ecommerce-logs-*/_search
{
  "query": {
    "bool": {
      "filter": [
        {
          "term": {
            "level": "CRITICAL"
          }
        },
        {
          "range": {
            "timestamp": {
              "gte": "now-24h"
            }
          }
        }
      ]
    }
  },
  "sort": [
    { "timestamp": "desc" }
  ]
}
```

**Pytania:**
1. Ile b≈Çƒôd√≥w krytycznych znaleziono?
2. Kt√≥ry serwis generuje najwiƒôcej b≈Çƒôd√≥w?

### Zadanie 2: Analiza wolnych request√≥w

**Wymaganie:** Znajd≈∫ wszystkie requesty, kt√≥re trwa≈Çy d≈Çu≈ºej ni≈º 3 sekundy.

```json
GET /ecommerce-logs-*/_search
{
  "query": {
    "range": {
      "response_time_ms": {
        "gte": 3000
      }
    }
  },
  "sort": [
    { "response_time_ms": "desc" }
  ],
  "size": 20
}
```

**Dodatkowe pytanie:**
Dodaj filtr, aby wykluczyƒá endpoint `/api/notify` (kt√≥ry czƒôsto jest wolny).

<details>
<summary>RozwiƒÖzanie</summary>

```json
{
  "query": {
    "bool": {
      "filter": [
        {
          "range": {
            "response_time_ms": {
              "gte": 3000
            }
          }
        }
      ],
      "must_not": [
        {
          "term": {
            "http.endpoint": "/api/notify"
          }
        }
      ]
    }
  }
}
```
</details>

### Zadanie 3: Nieudane p≈Çatno≈õci

**Wymaganie:** Znajd≈∫ wszystkie b≈Çƒôdy w payment-service zwiƒÖzane z p≈Çatno≈õciami.

```json
GET /ecommerce-logs-*/_search
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "service": "payment-service"
          }
        },
        {
          "term": {
            "level": "ERROR"
          }
        }
      ],
      "should": [
        {
          "match": {
            "message": "PAYMENT_DECLINED"
          }
        }
      ]
    }
  }
}
```

---

## Czƒô≈õƒá 4: Agregacje

### Zadanie 4: Liczba log√≥w per serwis

**Wymaganie:** Poka≈º liczbƒô log√≥w dla ka≈ºdego mikroserwisu.

```json
GET /ecommerce-logs-*/_search
{
  "size": 0,
  "aggs": {
    "services": {
      "terms": {
        "field": "service",
        "size": 10
      }
    }
  }
}
```

**Rozszerzenie:**
Dodaj podzia≈Ç na poziomy log√≥w w ka≈ºdym serwisie.

<details>
<summary>RozwiƒÖzanie</summary>

```json
{
  "size": 0,
  "aggs": {
    "services": {
      "terms": {
        "field": "service",
        "size": 10
      },
      "aggs": {
        "levels": {
          "terms": {
            "field": "level"
          }
        }
      }
    }
  }
}
```
</details>

### Zadanie 5: Wykres b≈Çƒôd√≥w w czasie

**Wymaganie:** Stw√≥rz histogram pokazujƒÖcy liczbƒô b≈Çƒôd√≥w (ERROR + CRITICAL) w czasie z interwa≈Çem 1 godzina.

```json
GET /ecommerce-logs-*/_search
{
  "size": 0,
  "query": {
    "terms": {
      "level": ["ERROR", "CRITICAL"]
    }
  },
  "aggs": {
    "errors_over_time": {
      "date_histogram": {
        "field": "timestamp",
        "fixed_interval": "1h"
      }
    }
  }
}
```

### Zadanie 6: Statystyki czas√≥w odpowiedzi

**Wymaganie:** Oblicz ≈õredni, minimalny, maksymalny i percentyle (50, 90, 95, 99) czas√≥w odpowiedzi.

```json
GET /ecommerce-logs-*/_search
{
  "size": 0,
  "aggs": {
    "response_time_stats": {
      "stats": {
        "field": "response_time_ms"
      }
    },
    "response_time_percentiles": {
      "percentiles": {
        "field": "response_time_ms",
        "percents": [50, 90, 95, 99]
      }
    }
  }
}
```

**Analiza:**
1. Jaki jest p95 (95th percentile)?
2. Czy sƒÖ outliery (warto≈õci znacznie powy≈ºej ≈õredniej)?

### Zadanie 7: Top 5 u≈ºytkownik√≥w z b≈Çƒôdami

**Wymaganie:** Znajd≈∫ 5 u≈ºytkownik√≥w, kt√≥rzy wygenerowali najwiƒôcej b≈Çƒôd√≥w.

```json
GET /ecommerce-logs-*/_search
{
  "size": 0,
  "query": {
    "terms": {
      "level": ["ERROR", "CRITICAL"]
    }
  },
  "aggs": {
    "top_users": {
      "terms": {
        "field": "user_id",
        "size": 5,
        "order": { "_count": "desc" }
      }
    }
  }
}
```

---

## Czƒô≈õƒá 5: Dashboard w Grafanie

### Zadanie 8: Konfiguracja Data Source

1. Dodaj Elasticsearch jako ≈∫r√≥d≈Ço danych w Grafanie
2. Parametry:
   - URL: `http://localhost:9200`
   - Index: `ecommerce-logs-*`
   - Time field: `timestamp`

### Zadanie 9: Tworzenie dashboardu

Utw√≥rz dashboard zawierajƒÖcy:

#### Panel 1: Total Logs (Stat)
- **Query**: Match all
- **Metric**: Count
- **Time range**: Last 24 hours

#### Panel 2: Error Count (Stat z thresholds)
- **Query**: `level:(ERROR OR CRITICAL)`
- **Metric**: Count
- **Thresholds**: 
  - Green: 0-10
  - Yellow: 10-50
  - Red: 50+

#### Panel 3: Logs Over Time (Time Series)
- **Query**: Match all
- **Group by**: Date Histogram (auto interval)
- **Split by**: level (terms aggregation)

#### Panel 4: Service Breakdown (Pie Chart)
- **Aggregation**: Terms on `service`
- **Metric**: Count

#### Panel 5: Response Time (Time Series)
- **Metric**: Percentiles (50, 90, 95) of `response_time_ms`
- **Group by**: Date Histogram

#### Panel 6: Recent Errors (Table)
- **Query**: `level:ERROR`
- **Fields**: timestamp, service, message, user_id
- **Size**: 50
- **Sort**: timestamp DESC

#### Panel 7: Error Stream (Logs Panel)
- **Query**: `level:(ERROR OR CRITICAL)`
- **Display**: timestamp, level, service, message

### Zadanie 10: Variables

Dodaj zmienne do dashboardu:

```
$service: Query variable
{"find": "terms", "field": "service"}

$level: Custom variable
INFO, WARNING, ERROR, CRITICAL

$time_range: Interval variable
15m, 1h, 6h, 24h, 7d
```

Zaktualizuj panele, aby wykorzystywa≈Çy zmienne:
```
service:$service AND level:$level
```

---

## Czƒô≈õƒá 6: Alerting

### Zadanie 11: Alert dla wysokiej liczby b≈Çƒôd√≥w

**Scenariusz:** Otrzymaj powiadomienie, gdy liczba b≈Çƒôd√≥w przekroczy 20 w ciƒÖgu 5 minut.

**Konfiguracja:**

1. Utw√≥rz panel z query: `level:ERROR`
2. W zak≈Çadce Alert dodaj regu≈Çƒô:
   ```
   WHEN last() OF query(A, 5m, now) IS ABOVE 20
   ```
3. Notification: Wy≈õlij na kana≈Ç (email/Slack)

### Zadanie 12: Alert dla wolnych request√≥w

**Scenariusz:** Alert gdy p95 czas√≥w odpowiedzi > 2000ms.

**Query:**
```json
{
  "aggs": {
    "response_time": {
      "percentiles": {
        "field": "response_time_ms",
        "percents": [95]
      }
    }
  }
}
```

**Condition:**
```
WHEN percentile_95 OF query(A, 5m, now) IS ABOVE 2000
```

---

## Pytania kontrolne

1. **Zapytania:**
   - Jaka jest r√≥≈ºnica miƒôdzy `match` a `term`?
   - Kiedy u≈ºywamy `must` vs `filter` w bool query?

2. **Agregacje:**
   - Dlaczego u≈ºywamy `size: 0` w zapytaniach agregacyjnych?
   - Co to sƒÖ percentyle i czym r√≥≈ºniƒÖ siƒô od ≈õredniej?

3. **Mapowania:**
   - Kiedy u≈ºywamy typu `keyword`, a kiedy `text`?
   - Po co u≈ºywamy multi-fields?

4. **Grafana:**
   - Jak dzia≈Ça zmienna `$__interval`?
   - Dlaczego Logs Panel jest lepszy ni≈º Table dla streamingu log√≥w?

---

## Podsumowanie

Po uko≈Ñczeniu ƒáwiczenia powiniene≈õ umieƒá:
- ‚úÖ Tworzyƒá index templates z w≈Ça≈õciwymi mapowaniami
- ‚úÖ Indeksowaƒá dane do Elasticsearch (bulk API)
- ‚úÖ Pisaƒá zapytania DSL (term, range, bool)
- ‚úÖ Tworzyƒá agregacje (terms, date_histogram, percentiles)
- ‚úÖ Konfigurowaƒá Elasticsearch w Grafanie
- ‚úÖ Budowaƒá dashboardy z r√≥≈ºnymi typami paneli
- ‚úÖ U≈ºywaƒá variables do dynamicznych filtr√≥w
- ‚úÖ Konfigurowaƒá alerty dla anomalii

Gratulacje! Uko≈Ñczy≈Çe≈õ modu≈Ç o Elasticsearch! üéâ
